{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5121317d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5121317d",
        "outputId": "44edfd80-3bea-4f94-95be-51e90d4dcf5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab1-2.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "a02eda80",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a02eda80"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79aca3c1",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "79aca3c1"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 1-2 — Text classification and evaluation methodology"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eff68c72",
      "metadata": {
        "id": "eff68c72"
      },
      "source": [
        "After this lab, you should be able to\n",
        "\n",
        "* Understand the distinction between training and test corpora, and why both are needed;\n",
        "* Understand the role of gold labels;\n",
        "* Implement a majority class baseline as a benchmark to compare other methods;\n",
        "* Implement nearest neighbor classification, and understand the role of distance metrics in its operation;\n",
        "* Compare multiple methods for acccuracy."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c06dcd6",
      "metadata": {
        "id": "2c06dcd6"
      },
      "source": [
        "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful, include\n",
        "\n",
        "* [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)\n",
        "* [`collections.Counter.most_common`](https://docs.python.org/3/library/collections.html#collections.Counter.most_common)\n",
        "* [`torch.float`](https://pytorch.org/docs/stable/tensors.html)\n",
        "* [`torch.Tensor.type`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html?highlight=torch%20tensor%20type#torch.Tensor.type)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aee274c0",
      "metadata": {
        "id": "aee274c0"
      },
      "source": [
        "# Preparation – Loading packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a60b2cd4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "source_hidden": true
        },
        "id": "a60b2cd4"
      },
      "outputs": [],
      "source": [
        "# Please do not change these imports because some hidden tests might depend on them.\n",
        "# You can add a cell below if you need to import anything else.\n",
        "import collections\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "from pprint import pprint\n",
        "import torch\n",
        "import wget"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff48706d",
      "metadata": {
        "id": "ff48706d"
      },
      "source": [
        "# The Federalist Papers\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/Federalist/federalist.jpg\" width=150 align=right />\n",
        "\n",
        "The _Federalist_ papers is a collection of 85 essays written pseudonymously by Alexander Hamilton, John Jay, and James Madison following the Constitutional Convention of 1787, promoting the ratification of the nascent United States Constitution.\n",
        "\n",
        "The authorship of many of the individual papers has been well established and acknowledged by the various authors, but a number of the papers have been contentious, with both Madison and Hamilton as possible authors. Determining the authorship of these disputed papers is a classic text classification problem, and one that has received great attention. The seminal work on the problem is that of [Mosteller and Wallace](http://www.historyofinformation.com/detail.php?entryid=4799), who applied then-novel statistical methods to the problem. In this lab, we'll use the _Federalist_ data to experiment with some of the ideas about distance metrics and classification methods that you've read about. (It's also an excuse to make some points about proper testing methodology.)\n",
        "\n",
        "Mosteller and Wallace used the frequencies of various words in the papers as the raw data for determining authorship. We've provided access to a heavily pre-digested version of this data. (If you're interested, you can find the raw data – all 85 papers – and the notebook used to generate the pre-digested data in the [course `data` github repository](https://github.com/nlp-236299/data/tree/master/Federalist).)\n",
        "\n",
        "Start by evaluating the cells below to load the data and view a sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3f9e158c",
      "metadata": {
        "id": "3f9e158c"
      },
      "outputs": [],
      "source": [
        "# Retrieve the Federalist data\n",
        "os.makedirs('data', exist_ok=True)\n",
        "wget.download('https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json', out='data/')\n",
        "# Read the json data into a data structure\n",
        "with open('data/federalist_data.json', 'r') as fin:\n",
        "    dataset = json.load(fin)\n",
        "# Convert counts to tensors of floats\n",
        "for example in dataset:\n",
        "    example['counts'] = torch.tensor(example['counts']).type(torch.float)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "b9d3d2a7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9d3d2a7",
        "outputId": "918d7694-eecf-469a-e281-e833d8f8a03a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers in the dataset: 85\n",
            "Some examples:\n",
            "[{'authors': 'Hamilton',\n",
            "  'counts': tensor([9., 6., 2., 0.]),\n",
            "  'number': '1',\n",
            "  'title': 'General Introduction'},\n",
            " {'authors': 'Jay',\n",
            "  'counts': tensor([8., 1., 0., 0.]),\n",
            "  'number': '2',\n",
            "  'title': 'Concerning Dangers from Foreign Force and Influence'},\n",
            " {'authors': 'Jay',\n",
            "  'counts': tensor([6., 0., 1., 0.]),\n",
            "  'number': '3',\n",
            "  'title': 'The Same Subject Continued: Concerning Dangers from Foreign Force '\n",
            "           'and Influence'}]\n"
          ]
        }
      ],
      "source": [
        "# View a sample of the data\n",
        "print(f\"Number of papers in the dataset: {len(dataset)}\")\n",
        "print(\"Some examples:\")\n",
        "pprint(dataset[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4114b980",
      "metadata": {
        "id": "4114b980"
      },
      "source": [
        "You'll see above that the dataset is a list of *examples*, one for each paper, each a dictionary providing the paper number, its title and author(s), and the raw counts for a few important words in the papers. From the last lab, you'll recognize the `counts` field as a bag-of-words representation of the document. The `counts` field is the document representation that we will be wanting to classify, and the `authors` field contains the pertinent class label for each example. \n",
        "\n",
        "For your reference, here are the words that were used to derive the counts:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ec3f4ceb",
      "metadata": {
        "id": "ec3f4ceb"
      },
      "outputs": [],
      "source": [
        "keywords = ['on', 'upon', 'there', 'whilst']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f07430a6",
      "metadata": {
        "id": "f07430a6"
      },
      "source": [
        "Thus in the first example paper, *Federalist 1*, there were 9 tokens of \"on\", 6 of \"upon\", 2 of \"there\", and none of \"whilst\". \n",
        "\n",
        "The `authors` field takes on various values. Here's a table of the frequency of each of the values. (This will come in handy later.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "55783589",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55783589",
        "outputId": "5e0b94f8-1b18-4a06-ed10-ef4245a989ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 51 (60.000%) Hamilton\n",
            "  5 (5.882%) Jay\n",
            " 15 (17.647%) Madison\n",
            "  3 (3.529%) Hamilton and Madison\n",
            " 11 (12.941%) Hamilton or Madison\n"
          ]
        }
      ],
      "source": [
        "# Generate a table of the number of papers by each author label\n",
        "cnt = collections.Counter(map(lambda ex: ex['authors'],\n",
        "                              dataset))\n",
        "for author, count in cnt.items():\n",
        "    print(f\"{count:3d} ({100.0*count/len(dataset):.3f}%) {author}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eabc43f3",
      "metadata": {
        "id": "eabc43f3"
      },
      "source": [
        "As you can see, some of the papers are of known authorship by one of Madison or Hamilton. We can use these as training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0c251bb4",
      "metadata": {
        "id": "0c251bb4"
      },
      "outputs": [],
      "source": [
        "# Extract the papers by either of Madison and Hamilton\n",
        "training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n",
        "                       dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7678cb8d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7678cb8d",
        "outputId": "a7bd42b6-37bf-4995-9db8-fe2ff7c2a878"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers in the dataset: 66\n",
            "Some examples:\n",
            "[{'authors': 'Hamilton',\n",
            "  'counts': tensor([9., 6., 2., 0.]),\n",
            "  'number': '1',\n",
            "  'title': 'General Introduction'},\n",
            " {'authors': 'Hamilton',\n",
            "  'counts': tensor([2., 4., 7., 0.]),\n",
            "  'number': '6',\n",
            "  'title': 'Concerning Dangers from Dissensions Between the States'},\n",
            " {'authors': 'Hamilton',\n",
            "  'counts': tensor([13., 11.,  9.,  0.]),\n",
            "  'number': '7',\n",
            "  'title': 'The Same Subject Continued: Concerning Dangers from Dissensions '\n",
            "           'Between the States'}]\n"
          ]
        }
      ],
      "source": [
        "# View a sample of the training data\n",
        "print(f\"Number of papers in the dataset: {len(training)}\")\n",
        "print(\"Some examples:\")\n",
        "pprint(training[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "996cfed0",
      "metadata": {
        "id": "996cfed0"
      },
      "source": [
        "Others of the papers are of ambiguous authorship. They are shown as having `'Hamilton or Madison'` as author. These will be the elements that we want to test our models on."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "063590c9",
      "metadata": {
        "id": "063590c9"
      },
      "outputs": [],
      "source": [
        "# Extract the papers of unknown authorship\n",
        "testing = list(filter(lambda ex: ex['authors'] == 'Hamilton or Madison',\n",
        "                      dataset))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "b96148ea",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b96148ea",
        "outputId": "02dffa68-02b5-4bc8-942f-e51dc3cb689d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of papers in the dataset: 11\n",
            "Some sample elements:\n",
            "[{'authors': 'Hamilton or Madison',\n",
            "  'counts': tensor([16.,  0.,  2.,  1.]),\n",
            "  'number': '49',\n",
            "  'title': 'Method of Guarding Against the Encroachments of Any One Department '\n",
            "           'of Government by Appealing to the People Through a Convention'},\n",
            " {'authors': 'Hamilton or Madison',\n",
            "  'counts': tensor([11.,  1.,  0.,  0.]),\n",
            "  'number': '50',\n",
            "  'title': 'Periodic Appeals to the People Considered'},\n",
            " {'authors': 'Hamilton or Madison',\n",
            "  'counts': tensor([21.,  0.,  2.,  2.]),\n",
            "  'number': '51',\n",
            "  'title': 'The Structure of the Government Must Furnish the Proper Checks and '\n",
            "           'Balances Between the Different Departments'}]\n"
          ]
        }
      ],
      "source": [
        "# View a sample of the data\n",
        "print(f\"Number of papers in the dataset: {len(testing)}\")\n",
        "print(\"Some sample elements:\")\n",
        "pprint(testing[:3])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d05a0d81",
      "metadata": {
        "id": "d05a0d81"
      },
      "source": [
        "# Models for text classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b3dd6460",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b3dd6460"
      },
      "source": [
        "We can think of a _model_ for a text classification problem as a function taking a test example and returning a class label for the test example. Generating the model will rely on a corpus of training data.\n",
        "\n",
        "With a model in hand, we can evaluate its _accuracy_ on a test corpus by computing the proportion of test examples that the model correctly classifies, that is, the model assigns to a test example the author that the test example specifies. Define a function `accuracy` that takes a test corpus (like `testing`) and a model (which is a function, remember), and returns the accuracy of the model on that corpus. \n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: accuracy\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "92dfa271",
      "metadata": {
        "id": "92dfa271"
      },
      "outputs": [],
      "source": [
        "#TODO -- Define the `accuracy` function.\n",
        "def accuracy(test_corpus, model):\n",
        "    \"\"\"Computes the accuracy of a model on a corpus.\n",
        "    Arguments:\n",
        "      `test_corpus`: a list of test examples, such as `testing`\n",
        "      `model`: a function whose input is an example from the corpus (such as \n",
        "              `testing[0]`, and whose output is the predicted author\n",
        "    Returns:\n",
        "      accuracy, a float number.\n",
        "    \"\"\"\n",
        "    sum = len(list(filter(lambda ex: ex['authors'] == model(ex),test_corpus)))\n",
        "    return sum/len(test_corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d373cd6b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d373cd6b"
      },
      "source": [
        "## Majority class classification\n",
        "\n",
        "An especially simple classification model labels each test example with whichever label happens to occur most frequently in the training data. It completely ignores the test example that it classifies!\n",
        "\n",
        "By examination of the table provided above, what is the majority class label for the training dataset?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: maj_class_label\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "4452070f",
      "metadata": {
        "id": "4452070f"
      },
      "outputs": [],
      "source": [
        "#TODO -- Set this variable to the majority class label for the training set.\n",
        "cntTraining = collections.Counter(map(lambda ex: ex['authors'],\n",
        "                              training))\n",
        "maj_class_label = cntTraining.most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "10ff7d04",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "10ff7d04",
        "outputId": "3cbf01df-d923-46ba-e68a-39ced5b5a69e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "grader.check(\"maj_class_label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2c0e429",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f2c0e429"
      },
      "source": [
        "Rather than determining the majority class by inspection, it's better to have a function to compute it for us. Define a function `majority_class_label` that returns the majority class label for a training set.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: majority_class_label\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "60ca6dbe",
      "metadata": {
        "id": "60ca6dbe"
      },
      "outputs": [],
      "source": [
        "#TODO -- Define the `majority_class_label` function.\n",
        "def majority_class_label(training):\n",
        "    \"\"\"Find the majority class label for a training set.\n",
        "    Arguments:\n",
        "      `training`: a list of training examples, such as `training`\n",
        "    Returns:\n",
        "      the majority class label, a string.\n",
        "    \"\"\"\n",
        "    cnt = collections.Counter(map(lambda ex: ex['authors'],\n",
        "                              training))\n",
        "    return cnt.most_common(1)[0][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "c3e59f3e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "c3e59f3e",
        "outputId": "da3cc969-2a4f-4b4c-876d-2cc2f745acf7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "grader.check(\"majority_class_label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c741d8a3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c741d8a3"
      },
      "source": [
        "What proportions of the *training* examples do you think would be classified correctly by the majority class model?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: maj_class_accuracy_guess\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "5d05f489",
      "metadata": {
        "id": "5d05f489"
      },
      "outputs": [],
      "source": [
        "#TODO -- Define this variable to be what you think the \n",
        "#        accuracy of the majority class model would be\n",
        "#        on the training data.\n",
        "maj_class_accuracy_guess =  cntTraining.most_common(1)[0][1]/len(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "4def5ca0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4def5ca0",
        "outputId": "caa3f684-c7c9-4200-83fa-a27a145f34ea"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "grader.check(\"maj_class_accuracy_guess\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35946410",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "35946410"
      },
      "source": [
        "Now define a function `majority_class` that takes a single argument (a test example) and returns the particular class label that is most frequent in the training data `training` (regardless of what the test example is).\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: majority_class\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "eff3697b",
      "metadata": {
        "id": "eff3697b"
      },
      "outputs": [],
      "source": [
        "#TODO - Define the `majority_class` model.\n",
        "def majority_class(example):\n",
        "    \"\"\"Defines a majority class model.\n",
        "    Arguments:\n",
        "      `example`: an example, such as `testing[0]`\n",
        "    Returns:\n",
        "      the majority class in the *training* set, a string.\n",
        "    \"\"\"\n",
        "    return majority_class_label(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "f70430b5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "f70430b5",
        "outputId": "647a6bd0-fa46-47aa-d3bb-2b24e236e775"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "grader.check(\"majority_class\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9424d51",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a9424d51"
      },
      "source": [
        "Now we can see how well this majority class model works by trying it out on some examples. Use the `accuracy` function to determine the model's accuracy when applied to the task of labeling the _training_ data.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: accuracy_maj_class_train\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "420ca7ed",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "420ca7ed",
        "outputId": "82d72a3a-ac72-48b1-ff1f-0856ad6e1333"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7727272727272727"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "#TODO -- Define `maj_class_on_train` to be the accuracy of the majority \n",
        "#        class model on the training data.\n",
        "accuracy_maj_class_train = accuracy(training,majority_class)\n",
        "accuracy_maj_class_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "76a995cd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "76a995cd",
        "outputId": "ee7419d7-d0b1-4b31-abf1-73a78edee692"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "grader.check(\"accuracy_maj_class_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "c3bd9569",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3bd9569",
        "outputId": "c538ab83-43a6-414d-8622-b2ea2e1114e0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the majority class model on training data: 0.773\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of the majority class model on training data: \"\n",
        "      f\"{accuracy_maj_class_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fad3ab8",
      "metadata": {
        "id": "3fad3ab8"
      },
      "source": [
        "Was your guess from above right?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0735ee19",
      "metadata": {
        "id": "0735ee19"
      },
      "source": [
        "## Nearest neighbor classification\n",
        "\n",
        "Recall that nearest neighbor classification classifies a test example with the label of the nearest training example. To calculate nearest neighbors, we need a distance metric between the representations of the documents. We will use two such metrics, familiar from the previous lab, for Euclidean distance and cosine distance.\n",
        "\n",
        "> Note: In order to allow full use of `torch` operations, these functions assume that the vectors are provided as tensors of type `float`. (That's why we tensorified the `counts` data as we loaded the dataset at the top  of this notebook.) When you call them, you'll want to make sure of this. They also return singleton tensors, not floats."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d94da69",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4d94da69"
      },
      "source": [
        "Just like in lab1-1, define a function `euclidean_distance` to compute the Euclidean distance between two vectors, and function `cosine_distance` to compute the cosine similarity between two vectors.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: euclidean_distance\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c5830a1a",
      "metadata": {
        "id": "c5830a1a"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def euclidean_distance(v1, v2):\n",
        "    return torch.linalg.norm(v1 - v2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "b9d3fce6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b9d3fce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "891e43f9-c0df-418b-dec4-73756426b455"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "grader.check(\"euclidean_distance\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "1e273d6e",
      "metadata": {
        "id": "1e273d6e"
      },
      "outputs": [],
      "source": [
        "def safe_acos(x):\n",
        "    \"\"\"Returns the arc cosine of `x`. Unlike `math.acos`, it \n",
        "       does not raise an exception for values of `x` out of range, \n",
        "       but rather clips `x` at -1..1, thereby avoiding math domain\n",
        "       errors in the case of numerical errors.\"\"\"\n",
        "    return math.acos(math.copysign(min(1.0, abs(x)), x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5106888a",
      "metadata": {
        "id": "5106888a"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def cosine_distance(v1, v2):\n",
        "    \"\"\"Returns the cosine distance between two vectors\"\"\"\n",
        "    return safe_acos( v1@v2 / (torch.linalg.norm(v1) * torch.linalg.norm(v2)))/math.pi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "32123087",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "32123087",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "cb6db0fb-4415-4fbb-a970-f43f57c9f689"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "grader.check(\"cosine_distance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d075064",
      "metadata": {
        "id": "5d075064"
      },
      "source": [
        "Here's an example of the use of these distance metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "541fb5c0",
      "metadata": {
        "id": "541fb5c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de0183c0-f843-4bfd-e08c-30e80273b80b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing on two different tensors\n",
            "Euclidean: 2.8284270763397217\n",
            "Cosine   : 0.05724914679911019\n",
            "\n",
            "Testing on two identical tensors\n",
            "Euclidean: 0.0\n",
            "Cosine   : 0.0\n"
          ]
        }
      ],
      "source": [
        "t1 = torch.tensor([1., 2.])\n",
        "t2 = torch.tensor([3., 4.])\n",
        "\n",
        "print(\"Testing on two different tensors\\n\"\n",
        "      f\"Euclidean: {euclidean_distance(t1, t2)}\\n\"\n",
        "      f\"Cosine   : {cosine_distance(t1, t2)}\\n\\n\"\n",
        "      \"Testing on two identical tensors\\n\"\n",
        "      f\"Euclidean: {euclidean_distance(t1, t1)}\\n\"\n",
        "      f\"Cosine   : {cosine_distance(t1, t1)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d1cb42",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "19d1cb42"
      },
      "source": [
        "### Generating nearest neighbor models\n",
        "\n",
        "To specify a nearest neighbor model, we need both a training corpus (like `training`) and a distance metric (like `euclidean_distance` or `cosine_distance` defined just above). \n",
        "\n",
        "Define a function called `define_nearest_neighbor` that takes a training corpus and a distance metric and returns a model -- that is, a function that classifies a single test example. The model should return the class _label_ of that training example whose _counts vector_ is closest to that of the test example according to the metric.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: define_nearest_neighbor\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "3b535c94",
      "metadata": {
        "id": "3b535c94"
      },
      "outputs": [],
      "source": [
        "#TODO -- Define this function that generates nearest neighbor models.\n",
        "def define_nearest_neighbor(corpus, metric):\n",
        "    \"\"\"Generates a nearest neighbor model from a training corpus and a\n",
        "    distance metric.\n",
        "    Arguments:\n",
        "      `corpus`: a training corpus, such as `training`\n",
        "      `metric`: a metric function which takes two tensors as input and \n",
        "                returns their distance, such as `euclidean_distance`\n",
        "    Returns:\n",
        "      a model, which is a function that takes in a test example (such as \n",
        "      `testing[0]`) and returns the author of the nearest example in the \n",
        "      training set, where distances are measured on the counts vector \n",
        "      using `metric`.\n",
        "    \"\"\"\n",
        "    return lambda example: sorted([(metric(example['counts'],item['counts']),item['authors']) for item in corpus])[0][1]\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "055dda0e",
      "metadata": {
        "id": "055dda0e"
      },
      "source": [
        "We can use the `define_nearest_neighbor` function to define two new models for nearest neighbor classification, one using Euclidean distance and one using cosine distance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "c66c8f4b",
      "metadata": {
        "id": "c66c8f4b"
      },
      "outputs": [],
      "source": [
        "nearest_neighbor_euclidean_model = \\\n",
        "    define_nearest_neighbor(training, euclidean_distance)\n",
        "\n",
        "nearest_neighbor_cosine_model = \\\n",
        "    define_nearest_neighbor(training, cosine_distance)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091732fe",
      "metadata": {
        "id": "091732fe"
      },
      "source": [
        "### Testing the nearest neighbor models on the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "60c40672",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "60c40672"
      },
      "source": [
        "How accurate are these models when used to label the training data (as we did for the majority class model above)? Use the `accuracy` function above to calculate the accuracy of `nearest_neighbor_euclidean_model` in labeling the _training_ data (not the test data), and similarly for `nearest_neighbor_cosine_model`.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: accuracy_train\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "a8f2ed50",
      "metadata": {
        "id": "a8f2ed50"
      },
      "outputs": [],
      "source": [
        "#TODO - Define the variable to be the calculated accuracy.\n",
        "accuracy_nn_euclidean_train = accuracy(training, nearest_neighbor_euclidean_model)\n",
        "accuracy_nn_cosine_train = accuracy(training, nearest_neighbor_cosine_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "04aa0e0e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "04aa0e0e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "615ed3a4-3281-41a1-9b1e-05b42a9cbf4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "grader.check(\"accuracy_train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c5c97371",
      "metadata": {
        "id": "c5c97371",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd4e226-ba70-4403-ae5a-bec5729f26f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the nearest neighbor euclidean model tested on training data: 1.000\n",
            "Accuracy of the nearest neighbor cosine model tested on training data: 1.000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of the nearest neighbor euclidean model tested on training data: \"\n",
        "      f\"{accuracy_nn_euclidean_train:.3f}\")\n",
        "print(f\"Accuracy of the nearest neighbor cosine model tested on training data: \"\n",
        "      f\"{accuracy_nn_cosine_train:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "62751621",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "62751621"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Does the performance of these classifiers on the training data seem to you to be representative of how good a classifier each is? Why or why not?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_1\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2c32e9d",
      "metadata": {
        "id": "d2c32e9d"
      },
      "source": [
        "**Answer:** No, the performance of each of these classifiers does not reflect how good the classifier is. The accuracy is expected to be high, since each classifier is exposed to the entire training set, and may even overfit it.In these cases, the accuracy is perfect (100%) because the value of every metric is minimal (0) when two vectors are identical, which always happens due to the accessibility to the entire training set. For measuring how good each classifier is, it is required to check the results with unknown examples (for instance – with a test set)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7feebb0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a7feebb0"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "#### Testing the nearest neighbor models on the testing data\n",
        "\n",
        "To get a better sense of how the nearest neighbor models perform, let's try them out on the testing data that we have. (Recall that the testing data in `testing` were the ambiguously-authored Federalist papers, where the `authors` field was `'Hamilton or Madison'`.)\n",
        "\n",
        "We start by looking in detail at the predictions generated by the two nearest neighbor models. Print out a table that lists, for each `testing` example, the paper number and the authors predicted under the nearest neighbor Euclidean model and the nearest neighbor cosine model. It might look something like\n",
        "```\n",
        "49 Madison  Madison \n",
        "50 Hamilton Madison \n",
        "51 Madison  Madison\n",
        "...\n",
        "```\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: print_table\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "1e3962f2",
      "metadata": {
        "id": "1e3962f2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ee18d92-a777-4750-c1bd-a4481d324ee7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "49\tMadison\tMadison\n",
            "50\tHamilton\tMadison\n",
            "51\tMadison\tMadison\n",
            "52\tMadison\tMadison\n",
            "53\tMadison\tMadison\n",
            "54\tMadison\tMadison\n",
            "55\tMadison\tHamilton\n",
            "56\tMadison\tMadison\n",
            "57\tMadison\tMadison\n",
            "62\tMadison\tMadison\n",
            "63\tMadison\tMadison\n"
          ]
        }
      ],
      "source": [
        "#TODO - Print out the requested table.\n",
        "for example in testing:\n",
        "    print(f\"{example['number']}\\t{nearest_neighbor_euclidean_model(example)}\\t{nearest_neighbor_cosine_model(example)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abf8a2c6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "abf8a2c6"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "What do you notice about the two models?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_2\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de8e1293",
      "metadata": {
        "id": "de8e1293"
      },
      "source": [
        "Sometime agree sometime don't. Trending the same?\n",
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1aacc2c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a1aacc2c"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### Testing the nearest neighbor models on the testing data\n",
        "\n",
        "Now use the `accuracy` function to calculate the accuracy of the two nearest neighbor models as you did above, but this time calculating accuracy on the *testing* corpus rather than the training corpus. (Expect to find a surprising result. Read ahead for an explanation if you're confused.)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: accuracy_test\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "f889fdf4",
      "metadata": {
        "id": "f889fdf4"
      },
      "outputs": [],
      "source": [
        "#TODO -- Define the variables to be, respectively, the calculated accuracy of the nearest \n",
        "#        neighbor Euclidean model and cosine model on the testing data.\n",
        "accuracy_nn_euclidean_test = accuracy(testing, nearest_neighbor_euclidean_model)\n",
        "accuracy_nn_cosine_test = accuracy(testing, nearest_neighbor_cosine_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "b1226f60",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b1226f60",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "1d3ccb25-8bf0-4fbe-c518-03398954496c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "grader.check(\"accuracy_test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "b19ba785",
      "metadata": {
        "id": "b19ba785",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2a40f02-feb5-41ba-cc7e-90ef21cc0557"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the nearest neighbor euclidean model tested on testing data: 0.000\n",
            "Accuracy of the nearest neighbor cosine model tested on testing data: 0.000\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of the nearest neighbor euclidean model tested on testing data: \"\n",
        "      f\"{accuracy_nn_euclidean_test:.3f}\")\n",
        "print(f\"Accuracy of the nearest neighbor cosine model tested on testing data: \"\n",
        "      f\"{accuracy_nn_cosine_test:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a8f191b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5a8f191b"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Does the performance of these classifiers on the testing data seem to you to be representative of how good a classifier each is? Why or why not?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_3\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c50ef2c5",
      "metadata": {
        "id": "c50ef2c5"
      },
      "source": [
        "**Answer:** Ne, the performance of these classifiers does not seem to be representative of how good each classifier is. Given an input example, each classifier return the label of the most similar known input example, so that label must be a label that exist in the training set. Since we have splitted the example such that all the test set examples have \"Hamilton or Madison\" label, and there is no example in the training set with such a label – these classifiers always fail for these example. A fair mesurment requires a more even splitting of examples according to the label."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e25c4441",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e25c4441"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "### The importance of gold labels\n",
        "\n",
        "In order to evaluate the accuracy of the nearest neighbor model – and any model – we need to have the correct labels for the testing corpus, the so-called _gold_ labels. What shall we use for gold labels? Mosteller and Wallace's much more extensive analysis concluded that all of the papers of ambiguous origin were penned by Madison, so we'll use that. We should use a version of the `testing` corpus with the gold labels. \n",
        "\n",
        "Write some code to generate a version of the testing corpus with the gold labels.\n",
        "\n",
        "> Hint: In defining `testing_gold`, you'll want to be careful not to change `testing`. Otherwise, some unit tests that use `testing` may fail. The `copy.deepcopy` function may be useful.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: get_gold\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "4289faec",
      "metadata": {
        "id": "4289faec"
      },
      "outputs": [],
      "source": [
        "#TODO - Write code that defines `testing_gold`, which is the same\n",
        "# as `testing` except that it has the correct gold labels.\n",
        "# Note: be careful to not change `testing`.\n",
        "testing_gold = copy.deepcopy(testing)\n",
        "for example in testing_gold:\n",
        "    example['authors'] = 'Madison'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "587ee5f5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "587ee5f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "2f904d45-7f08-48a2-9108-0296808a7ba0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "grader.check(\"get_gold\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1225de57",
      "metadata": {
        "id": "1225de57"
      },
      "source": [
        "Now we can rerun the accuracy calculations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "281cfd9e",
      "metadata": {
        "id": "281cfd9e"
      },
      "outputs": [],
      "source": [
        "accuracy_nn_euclidean_test_with_gold = accuracy(testing_gold, nearest_neighbor_euclidean_model)\n",
        "accuracy_nn_cosine_test_with_gold = accuracy(testing_gold, nearest_neighbor_cosine_model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "5e4c211c",
      "metadata": {
        "id": "5e4c211c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2abd25d6-4afc-45c4-9bbe-20f070b54328"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the nearest neighbor euclidean model tested on testing data: 0.909\n",
            "Accuracy of the nearest neighbor cosine model tested on testing data: 0.909\n"
          ]
        }
      ],
      "source": [
        "print(f\"Accuracy of the nearest neighbor euclidean model tested on testing data: \"\n",
        "      f\"{accuracy_nn_euclidean_test_with_gold:.3f}\")\n",
        "print(f\"Accuracy of the nearest neighbor cosine model tested on testing data: \"\n",
        "      f\"{accuracy_nn_cosine_test_with_gold:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7800f34",
      "metadata": {
        "id": "a7800f34"
      },
      "source": [
        "Do these results make more sense?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98cf23d3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "98cf23d3"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e5577014",
      "metadata": {
        "id": "e5577014"
      },
      "source": [
        "**Answer:**\n",
        "  * It was decent in length, but the dependency on lab1-1 code made it much harder to finish the notebook at class.\n",
        "  * The reading were appropriate but they seem to be irrelevant to the notebook.\n",
        "  * The instructions regarding the test set and traning set were very confusing.\n",
        "  * We think that this notebook is too complicated to complete for its goals."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4820a428",
      "metadata": {
        "id": "4820a428"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of lab 1-2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac79d523",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ac79d523"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "69ce6674",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "69ce6674",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "d7020ea7-3e87-4c75-f290-9ffee5735408"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "accuracy_maj_class_train:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "accuracy_test:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "accuracy_test_gold:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "accuracy_train:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "cosine_distance:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "euclidean_distance:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "get_gold:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "maj_class_accuracy_guess:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "maj_class_label:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "majority_class:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "majority_class_label:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>accuracy_maj_class_train:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>accuracy_test:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>accuracy_test_gold:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>accuracy_train:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>cosine_distance:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>euclidean_distance:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>get_gold:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>maj_class_accuracy_guess:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>maj_class_label:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>majority_class:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>majority_class_label:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "otter-latest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 1-2: Text classification and evaluation methodology",
    "vscode": {
      "interpreter": {
        "hash": "4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}