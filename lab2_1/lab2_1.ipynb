{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c20f0c5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "9c20f0c5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fe64935-5509-4fed-867f-91fd81ab7733"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab2-1.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f017a9aa",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f017a9aa"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "ef4c3c8a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "ef4c3c8a"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1938d59e",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [
          "remove_for_latex"
        ],
        "id": "1938d59e"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "718a78a5",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "718a78a5"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 2-1 – Language modeling with $n$-grams"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ae0b80b",
      "metadata": {
        "id": "9ae0b80b"
      },
      "source": [
        "We turn from tasks that _classify_ texts – mapping texts into a finite set of classes – to tasks that _model_ texts by providing a full probability distribution over texts (or providing a similar scoring metric). Such _language models_ attempt to answer the question \"How likely is a token sequence to be generated as an instance of the language?\".\n",
        "\n",
        "We'll start with $n$-gram language models. Given a token sequence $\\vect{w} = w_1, w_2, \\ldots, w_N$, its probability $\\Prob(w_1, w_2, \\ldots, w_N)$ can be calculated using the chain rule of probability:\n",
        "\n",
        "$$\\Prob(A, B \\given \\theta)= \\Prob(A \\given \\theta) \\cdot \\Prob(B \\given A, \\theta) $$\n",
        "\n",
        "Thus, \n",
        "\n",
        "\\begin{align}\n",
        "\\Prob(w_1, w_2, \\ldots, w_N) & = \\Prob(w_1) \\cdot \\Prob(w_2, \\ldots, w_N \\given w_1) \\\\\n",
        "& = \\Prob(w_1) \\cdot \\Prob(w_2 \\given w_1) \\cdot \\Prob(w_3, \\ldots, w_N \\given w_1, w_2) \\\\\n",
        "& \\cdots \\\\\n",
        "& = \\prod_{i=1}^N \\Prob (w_i \\given w_1, \\cdots, w_{i-1}) \\\\\n",
        "& \\approx \\prod_{i=1}^N \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})\\tag{1}\n",
        "\\end{align}\n",
        "\n",
        "In the last step, we replace the probability $\\Prob (w_i \\given w_1, \\cdots, w_{i-1})$, which conditions $w_i$ on _all_ of the preceding tokens, with $\\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})$, which conditions $w_i$ only on the $n-1$ preceding tokens. We call the $n-1$ preceding tokens ($w_{i-n+1}, \\cdots, w_{i-1}$) the _context_ and $w_i$ the _target_. Taken together, these $n$ tokens form an $n$-gram, hence the term _$n$-gram model_.\n",
        "\n",
        "In this lab you'll work with $n$-gram models: generating them, sampling from them, and scoring held-out texts according to them. We'll find some problems with $n$-gram models as language models:\n",
        "\n",
        "1. They are profligate with memory.\n",
        "2. They are sensitive to very limited context.\n",
        "3. They don't generalize well across similar words.\n",
        "\n",
        "In the next lab, we'll explore neural models to address these failings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20290aa1",
      "metadata": {
        "id": "20290aa1"
      },
      "source": [
        "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
        "\n",
        "* [`itertools.product`](https://docs.python.org/3.8/library/itertools.html#itertools.product)\n",
        "* [`list`](https://docs.python.org/3/library/functions.html#func-list)\n",
        "* [`tuple`](https://docs.python.org/3/library/functions.html#func-tuple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "091ae3b5",
      "metadata": {
        "id": "091ae3b5"
      },
      "source": [
        "# Preparation – Loading packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3de3a8a4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3de3a8a4"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "import math\n",
        "import random\n",
        "import re\n",
        "import wget\n",
        "\n",
        "from collections import defaultdict, Counter\n",
        "from sys import getsizeof\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt', quiet=True) # this module is used to tokenize the text\n",
        "\n",
        "# Set random seeds\n",
        "SEED = 1234\n",
        "random.seed(SEED)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "143b71f0",
      "metadata": {
        "id": "143b71f0"
      },
      "outputs": [],
      "source": [
        "# Some utilities to manipulate the corpus\n",
        "\n",
        "def preprocess(text):\n",
        "    \"\"\"Strips #comments and empty lines from a string\n",
        "    \"\"\"\n",
        "    result = []\n",
        "    for line in text.split(\"\\n\"):\n",
        "        line = line.strip()              # trim whitespace\n",
        "        line = re.sub('#.*$', '', line)  # trim comments\n",
        "        if line != '':                   # drop blank lines\n",
        "            result.append(line)\n",
        "    return result\n",
        "\n",
        "def nltk_normpunc_tokenize(str):\n",
        "    return nltk.tokenize.word_tokenize(str.lower())\n",
        "\n",
        "\n",
        "def geah_tokenize(lines):\n",
        "    \"\"\"Specialized tokenizer for GEaH corpus handling speaker IDs\"\"\"\n",
        "    result = []\n",
        "    for line in lines:\n",
        "        # tokenize\n",
        "        tokens = nltk_normpunc_tokenize(line)\n",
        "        # revert the speaker ID token\n",
        "        if tokens[0] == \"sam\":\n",
        "            tokens[0] = \"SAM:\"\n",
        "        elif tokens[0] == \"guy\":\n",
        "            tokens[0] = \"GUY:\"\n",
        "        else:\n",
        "            raise ValueError(\"format problem - bad speaker ID\")\n",
        "        # add a start of sentence token\n",
        "        result += [\"<s>\"] + tokens\n",
        "    return result\n",
        "                    \n",
        "def postprocess(tokens):\n",
        "    \"\"\"Converts `tokens` to a string with one sentence per line\"\"\"\n",
        "    return ' '.join(tokens)\\\n",
        "              .replace(\"<s> \", \"\\n\")\n",
        "\n",
        "# Read the GEaH data and preprocess into training and test streams of tokens\n",
        "geah_filename = (\"https://github.com/nlp-236299/data/raw/master/Seuss/\"\n",
        "                 \"seuss - 1960 - green eggs and ham.txt\")\n",
        "os.makedirs('data', exist_ok=True)\n",
        "wget.download(geah_filename, out=\"data/\")\n",
        "\n",
        "def split(list, portions, offset):\n",
        "    \"\"\"Splits `list` into a \"large\" and a \"small\" part, returning them as a pair.\n",
        "    \n",
        "    The two parts are formed by partitioning `list` into `portions` disjoint pieces.\n",
        "    The small part is the piece at index `offset`; the large part is the remainder.\n",
        "    \"\"\"\n",
        "    return ([list[i] for i in range(0, len(list)) if i % portions != offset],\n",
        "            [list[i] for i in range(0, len(list)) if i % portions == offset])\n",
        "\n",
        "with open(\"data/seuss - 1960 - green eggs and ham.txt\", 'r') as fin:\n",
        "    lines = preprocess(fin.read())\n",
        "    train_lines, test_lines = split(lines, 12, 0)\n",
        "    train_tokens = geah_tokenize(train_lines)\n",
        "    test_tokens = geah_tokenize(test_lines)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1e0ded6",
      "metadata": {
        "id": "e1e0ded6"
      },
      "source": [
        "We've already loaded in the text of _Green Eggs and Ham_ for you and split it (about 90%/10%) into two token sequences, `train_tokens` and `test_tokens`. Here's a preview:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a75ef55a",
      "metadata": {
        "id": "a75ef55a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "437c7eba-6d5a-4dd7-c996-18fe5b9bf9da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'SAM:', 'i', 'am', 'sam', '.', '<s>', 'SAM:', 'sam', 'i', 'am', '.', '<s>', 'GUY:', 'that', 'sam-i-am', '!', '<s>', 'GUY:', 'that', 'sam-i-am', '!', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'that', 'sam-i-am', '!', '<s>', 'SAM:', 'do', 'you', 'like', 'green', 'eggs', 'and', 'ham', '?', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'them', ',', 'sam-i-am']\n",
            "\n",
            "SAM: i am sam . \n",
            "SAM: sam i am . \n",
            "GUY: that sam-i-am ! \n",
            "GUY: that sam-i-am ! \n",
            "GUY: i do not like that sam-i-am ! \n",
            "SAM: do you like green eggs and ham ? \n",
            "GUY: i do not like them , sam-i-am\n"
          ]
        }
      ],
      "source": [
        "print(train_tokens[:50])\n",
        "print(postprocess(train_tokens[:50]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a69f9cff",
      "metadata": {
        "id": "a69f9cff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1fea4b80-6470-4ec6-c84c-f5296357e5d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'SAM:', 'i', 'am', 'sam', '.', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham', '.', '<s>', 'GUY:', 'not', 'in', 'a', 'box', '.', '<s>', 'SAM:', 'eat', 'them', '!', '<s>', 'GUY:', 'i', 'do', 'not', 'like', 'them', 'with', 'a', 'mouse', '.', '<s>', 'GUY:', 'not', 'in', 'a', 'car', '!', '<s>', 'SAM:', 'in']\n",
            "\n",
            "SAM: i am sam . \n",
            "GUY: i do not like green eggs and ham . \n",
            "GUY: not in a box . \n",
            "SAM: eat them ! \n",
            "GUY: i do not like them with a mouse . \n",
            "GUY: not in a car ! \n",
            "SAM: in\n"
          ]
        }
      ],
      "source": [
        "print(test_tokens[:50])\n",
        "print(postprocess(test_tokens[:50]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4739aa09",
      "metadata": {
        "id": "4739aa09"
      },
      "source": [
        "We extract the vocabulary from the training text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6ac42697",
      "metadata": {
        "id": "6ac42697",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c56447a3-bfe0-4979-9cae-3acfd84307cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['do', 'mouse', 'they', 'are', 'house', 'may', 'try', '...', '<s>', 'GUY:', 'be', 'dark', 'tree', 'on', 'boat', 'goat', ',', 'not', 'anywhere', 'would', 'or', 'am', 'that', '?', 'me', 'eggs', 'SAM:', 'the', 'here', 'green', 'if', 'see', 'box', 'and', 'i', '!', 'ham', '.', 'you', 'them', 'let', 'there', 'good', 'thank', 'so', 'a', 'car', 'say', 'eat', 'with', 'will', 'sam', 'like', 'train', 'rain', 'in', 'could', 'fox', 'sam-i-am']\n"
          ]
        }
      ],
      "source": [
        "# Extract vocabulary from dataset\n",
        "vocabulary = list(set(train_tokens))\n",
        "print(vocabulary)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c68ecec",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3c68ecec"
      },
      "source": [
        "# Generating $n$-grams\n",
        "\n",
        "The $n$-grams in a text are the contiguous subsequences of $n$ tokens. (We'll implement them as Python tuples.) In theory, any sequence of $n$ tokens is a potential $n$-gram type. Let's generate a list of all the possible $n$-gram types over a vocabulary. (Notice how the type/token distinction is useful for talking about $n$-grams, just as it is for words.)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: all_ngrams\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee014aaa",
      "metadata": {
        "id": "ee014aaa"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def all_ngrams(vocabulary, n):\n",
        "    \"\"\"Returns a list of all `n`-long *tuples* of elements of the `vocabulary`.\n",
        "    \n",
        "    For instance,  \n",
        "        >>> all_ngrams([\"one\", \"two\"], 3)\n",
        "        [('one', 'one', 'one'),\n",
        "         ('one', 'one', 'two'),\n",
        "         ('one', 'two', 'one'),\n",
        "         ('one', 'two', 'two'),\n",
        "         ('two', 'one', 'one'),\n",
        "         ('two', 'one', 'two'),\n",
        "         ('two', 'two', 'one'),\n",
        "         ('two', 'two', 'two')]\n",
        "         \n",
        "    Order of returned list is not specified or guaranteed.\n",
        "    When `n` is 0, returns `[()]`.\n",
        "    \"\"\"\n",
        "    return list(itertools.product(vocabulary,repeat=n))\n",
        "    ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "955db0ec",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "955db0ec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "a74edb22-5326-4cc7-f302-2a4cdd9814e8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "grader.check(\"all_ngrams\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "139789e9",
      "metadata": {
        "id": "139789e9"
      },
      "source": [
        "We can generate a list of all of the $n$-grams (tokens, not types) in a text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baec7700",
      "metadata": {
        "id": "baec7700"
      },
      "outputs": [],
      "source": [
        "def ngrams(tokens, n):\n",
        "    \"\"\"Returns a list of all `n`-gram instances in a list of `tokens`, in order.\n",
        "    \n",
        "    For instance, \n",
        "    \n",
        "    >>> ngrams(nltk_normpunc_tokenize('I am Sam! Sam I am.'), 3)\n",
        "    [('i', 'am', 'sam'),\n",
        "     ('am', 'sam', '!'),\n",
        "     ('sam', '!', 'sam'),\n",
        "     ('!', 'sam', 'i'),\n",
        "     ('sam', 'i', 'am'),\n",
        "     ('i', 'am', '.')]\n",
        "    \"\"\"\n",
        "    return [tuple(tokens[i : i + n])\n",
        "            for i in range(0, len(tokens) - n + 1)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc77430b",
      "metadata": {
        "id": "cc77430b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc4a629-3d2e-4681-c9c2-6260bdcee43e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['<s>', 'SAM:', 'i', 'am', 'sam', '.']\n",
            "[('<s>', 'SAM:', 'i'), ('SAM:', 'i', 'am'), ('i', 'am', 'sam'), ('am', 'sam', '.')]\n"
          ]
        }
      ],
      "source": [
        "print (train_tokens[:6])\n",
        "print (ngrams(train_tokens[:6], 3))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d8147d8",
      "metadata": {
        "id": "2d8147d8"
      },
      "source": [
        "# Counting $n$-grams\n",
        "\n",
        "We conceptualize an $n$-gram as having two parts:\n",
        "\n",
        "* The _context_ is the first $n-1$ tokens in the $n$-gram.\n",
        "* The _target_ is the final token in the $n$-gram.\n",
        "\n",
        "An $n$-gram language model specifies a probability for each $n$-gram type. We'll implement a model as a 2-D dictionary, indexed first by context and then by target, providing the probability for the $n$-gram.\n",
        "\n",
        "We start by generating a similar data structure for counting up the $n$-grams in a token sequence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7a14493",
      "metadata": {
        "id": "b7a14493"
      },
      "outputs": [],
      "source": [
        "def ngram_counts(vocabulary, tokens, n):\n",
        "    \"\"\"Returns a dictionary of counts of the `n`-grams in `tokens`.\n",
        "    \n",
        "    The dictionary is structured with first index by (n-1)-gram context\n",
        "    and second index by the final target token.\n",
        "    \"\"\"\n",
        "    context_dict = defaultdict(lambda: defaultdict(int))\n",
        "    # zero all ngrams\n",
        "    for context in all_ngrams(vocabulary, n - 1):\n",
        "        for target in vocabulary:\n",
        "            context_dict[context][target] = 0\n",
        "    # add counts for attested ngrams\n",
        "    for ngram, count in Counter(ngrams(tokens, n)).items():\n",
        "        context_dict[ngram[:-1]][ngram[-1]] = count\n",
        "    return context_dict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5684bc6a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5684bc6a"
      },
      "source": [
        "Use the `ngram_counts` function to generate count data structures for unigrams, bigrams, and trigrams for the _Green Eggs and Ham_ training text.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: ngram_counts\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a162effe",
      "metadata": {
        "id": "a162effe"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "unigram_counts = ngram_counts(vocabulary,train_tokens,1)\n",
        "bigram_counts = ngram_counts(vocabulary,train_tokens,2)\n",
        "trigram_counts = ngram_counts(vocabulary,train_tokens,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "284da456",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "284da456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "766ac28b-7686-4df0-c535-2888f5d94a71"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "grader.check(\"ngram_counts\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8cf1971",
      "metadata": {
        "id": "c8cf1971"
      },
      "source": [
        "Check your work by examining the total count of unigrams, bigrams, and trigrams. Do the numbers make sense?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd073b55",
      "metadata": {
        "id": "cd073b55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ebda75e-1b3f-4afa-de04-3da3d5b39d78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:     1145\n",
            "Unigrams:     59\n",
            "Bigrams:    3481\n",
            "Trigrams: 205379\n"
          ]
        }
      ],
      "source": [
        "# Calculate total counts of tokens, unigrams, bigrams, and trigrams\n",
        "token_count = len(train_tokens)\n",
        "unigram_count = sum(len(unigram_counts[cntxt]) for cntxt in unigram_counts)\n",
        "bigram_count = sum(len(bigram_counts[cntxt]) for cntxt in bigram_counts)\n",
        "trigram_count = sum(len(trigram_counts[cntxt]) for cntxt in trigram_counts)               \n",
        "\n",
        "# Report on the totals\n",
        "print(f\"Tokens:   {token_count:6}\\n\"\n",
        "      f\"Unigrams: {unigram_count:6}\\n\"\n",
        "      f\"Bigrams:  {bigram_count:6}\\n\"\n",
        "      f\"Trigrams: {trigram_count:6}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa3c9f16",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fa3c9f16"
      },
      "source": [
        "## Calculating $n$-gram probabilities\n",
        "\n",
        "We can convert the counts into a probability model by _normalizing_ the counts. Given an $n$-gram type $w_1, w_2, \\ldots, w_n$, instead of storing the count $\\cnt{w_1, w_2, \\ldots, w_n}$, we store an estimate of the probability \n",
        "\n",
        "\\begin{align*}\n",
        "  \\Pr(w_n \\given w_1, w_2, \\ldots, w_{n-1})\n",
        "  & \\approx \\frac{\\cnt{w_1, w_2, \\ldots, w_n}}{\\cnt{w_1, w_2, \\ldots, w_{n-1}}} \\\\\n",
        "  & = \\frac{\\cnt{w_1, w_2, \\ldots, w_n}}{\\sum_{w'} \\cnt{w_1, w_2, \\ldots, w_{n-1}, w'}}\n",
        "\\end{align*}\n",
        "\n",
        "that is, the ratio of the count of the $n$-gram and the sum of the counts of all $n$-grams with the same context. Fortunately, all of those counts are already stored in the count data structures we've already built. \n",
        "\n",
        "Write a function that takes an $n$-gram count data structure and returns an $n$-gram probability data structure. As with the counts, the probabilities should be stored indexed first by context and then by target.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: ngram_model\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35b2bf50",
      "metadata": {
        "id": "35b2bf50"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "\n",
        "def ngram_model(ngram_counts):\n",
        "    \"\"\"Returns an n-gram probability model calculated by normalizing the \n",
        "       provided `ngram-counts` dictionary\n",
        "    \"\"\"\n",
        "    ngram_probabilities = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for context,targets in ngram_counts.items():\n",
        "        context_count = sum([target_count for target,target_count in targets.items()])\n",
        "        for target,target_count in targets.items():\n",
        "            if(context_count>0):\n",
        "                ngram_probabilities[context][target] = target_count/context_count\n",
        "    return ngram_probabilities          "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "980fcd93",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "980fcd93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "3d2bb7c6-9993-480f-d3ca-8e042d048472"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "grader.check(\"ngram_model\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34fcbf6f",
      "metadata": {
        "id": "34fcbf6f"
      },
      "source": [
        "We can now build some $n$-gram models – unigram, bigram, and trigram – based on the counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b493a253",
      "metadata": {
        "id": "b493a253"
      },
      "outputs": [],
      "source": [
        "unigram_model = ngram_model(unigram_counts)\n",
        "bigram_model = ngram_model(bigram_counts)\n",
        "trigram_model = ngram_model(trigram_counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7877f449",
      "metadata": {
        "id": "7877f449"
      },
      "source": [
        "# Space considerations\n",
        "\n",
        "For the most part, we aren't too concerned about matters of time or space efficiency, though these are crucial issues in the engineering of NLP systems. But the size of $n$-gram models merits consideration, looking especially at their size as $n$ grows. We can use Python's [`sys.getsizeof`](https://docs.python.org/3/library/sys.html#sys.getsizeof) function to get a rough sense of the size of the models we've been working with."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98765079",
      "metadata": {
        "id": "98765079",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37925ca7-3f44-4083-8491-3c69ae975549"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokens:    10328\n",
            "Unigrams:    240\n",
            "Bigrams:    2280\n",
            "Trigrams:   9320\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tokens:   {getsizeof(train_tokens):6}\\n\"\n",
        "      f\"Unigrams: {getsizeof(unigram_model):6}\\n\"\n",
        "      f\"Bigrams:  {getsizeof(bigram_model):6}\\n\"\n",
        "      f\"Trigrams: {getsizeof(trigram_model):6}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cd0368",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "c0cd0368"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What do these sizes tell you about the memory usage of $n$-gram models? With a larger vocabulary of, say, 10,000 words, would it be practical to run, say, 5-gram models on your laptop?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_sizes\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "454c9746",
      "metadata": {
        "id": "454c9746"
      },
      "source": [
        "The memory usage grows as N grows and it seem to trend exponentially with the n-gram model.  \n",
        "Given that executing: `getsizeof(ngram_model(ngram_counts(vocabulary,train_tokens,5)))` take quite a while on our dataset, which only has 59 words in the vocabulary, It seems quite infeasable to execute a similar 5 gram model with 10,000 words vocabulary."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "719f573e",
      "metadata": {
        "id": "719f573e"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# Sampling from an $n$-gram model\n",
        "\n",
        "We have cleverly constructed the models to index by context. This allows us to sample a word given its context. For instance, in the trigram context `(\"<s>\", \"SAM:\")`, the following probability distribution captures which words can come next and with what probability:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbf12d5",
      "metadata": {
        "id": "6cbf12d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80be9e32-e93e-4ba5-8661-a6a45d1cd5fe"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "defaultdict(int,\n",
              "            {'do': 0.029411764705882353,\n",
              "             'mouse': 0.0,\n",
              "             'they': 0.0,\n",
              "             'are': 0.0,\n",
              "             'house': 0.0,\n",
              "             'may': 0.0,\n",
              "             'try': 0.08823529411764706,\n",
              "             '...': 0.0,\n",
              "             '<s>': 0.0,\n",
              "             'GUY:': 0.0,\n",
              "             'be': 0.0,\n",
              "             'dark': 0.0,\n",
              "             'tree': 0.0,\n",
              "             'on': 0.0,\n",
              "             'boat': 0.0,\n",
              "             'goat': 0.0,\n",
              "             ',': 0.0,\n",
              "             'not': 0.0,\n",
              "             'anywhere': 0.0,\n",
              "             'would': 0.2647058823529412,\n",
              "             'or': 0.0,\n",
              "             'am': 0.0,\n",
              "             'that': 0.0,\n",
              "             '?': 0.0,\n",
              "             'me': 0.0,\n",
              "             'eggs': 0.0,\n",
              "             'SAM:': 0.0,\n",
              "             'the': 0.0,\n",
              "             'here': 0.058823529411764705,\n",
              "             'green': 0.0,\n",
              "             'if': 0.0,\n",
              "             'see': 0.0,\n",
              "             'box': 0.0,\n",
              "             'and': 0.029411764705882353,\n",
              "             'i': 0.029411764705882353,\n",
              "             '!': 0.0,\n",
              "             'ham': 0.0,\n",
              "             '.': 0.0,\n",
              "             'you': 0.14705882352941177,\n",
              "             'them': 0.0,\n",
              "             'let': 0.0,\n",
              "             'there': 0.0,\n",
              "             'good': 0.0,\n",
              "             'thank': 0.0,\n",
              "             'so': 0.029411764705882353,\n",
              "             'a': 0.11764705882352941,\n",
              "             'car': 0.0,\n",
              "             'say': 0.029411764705882353,\n",
              "             'eat': 0.029411764705882353,\n",
              "             'with': 0.0,\n",
              "             'will': 0.0,\n",
              "             'sam': 0.029411764705882353,\n",
              "             'like': 0.0,\n",
              "             'train': 0.0,\n",
              "             'rain': 0.0,\n",
              "             'in': 0.029411764705882353,\n",
              "             'could': 0.08823529411764706,\n",
              "             'fox': 0.0,\n",
              "             'sam-i-am': 0.0})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "trigram_model[(\"<s>\", \"SAM:\")]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41d0d787",
      "metadata": {
        "id": "41d0d787"
      },
      "source": [
        "We can sample a single token according to this probability distribution. Here's one way to do so."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d33c9279",
      "metadata": {
        "id": "d33c9279"
      },
      "outputs": [],
      "source": [
        "def sample(model, context):\n",
        "    \"\"\"Returns a token sampled from the `model` assuming the `context`\"\"\"\n",
        "    distribution = model[context]\n",
        "    prob_remaining = random.random()\n",
        "    for token, prob in sorted(distribution.items()):\n",
        "        if prob_remaining < prob:\n",
        "            return token\n",
        "        else:\n",
        "            prob_remaining -= prob\n",
        "    raise ValueError"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b172efc6",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b172efc6"
      },
      "source": [
        "We can extend the sampling to a sequence of words by updating the context as we sample each word.\n",
        "\n",
        "Define a function `sample_sequence` that performs this sampling of a sequence. It's given a model and a starting context and begins by sampling the first token based on the starting context, then updates the starting context to reflect the word just sampled, repeating the process until a specified number of tokens have been sampled.\n",
        "\n",
        "> Hint: You might find function [`list`](https://docs.python.org/3/library/functions.html#func-list) helpful for converting immutable tuples to lists, and conversely [`tuple`](https://docs.python.org/3/library/functions.html#func-tuple) helpful for converting lists to tuples.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: sample_sequence\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d41c3d1",
      "metadata": {
        "id": "4d41c3d1"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def sample_sequence(model, start_context, count=100):\n",
        "    \"\"\"Returns a sequence of `count` tokens sampled successively\n",
        "       from the `model` *following the `start_context`*.\n",
        "       The length of the returned list should be `count+len(start_context)`.\n",
        "    \"\"\"\n",
        "    random.seed(SEED) # for reproducibility, do not change\n",
        "    current_context = start_context\n",
        "    n = len(start_context)\n",
        "    sequence = list(start_context)\n",
        "    for i in range(count):\n",
        "        token = sample(model,current_context)\n",
        "        sequence = list(sequence)+[token]\n",
        "        if n > 0:\n",
        "            current_context_aslist = (list(current_context)+[token])[-n:]\n",
        "            current_context = tuple(current_context_aslist)\n",
        "        else:\n",
        "            current_context = ()\n",
        "    return sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f64abe4f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f64abe4f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "e538dcad-ef8e-41c6-c2a7-c2aa54607205"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "grader.check(\"sample_sequence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76978bfc",
      "metadata": {
        "id": "76978bfc"
      },
      "source": [
        "Let's try it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28dd5485",
      "metadata": {
        "id": "28dd5485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5e57bc2-cfe3-45a1-ff37-41177411c32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "would anywhere ! tree with i like . not \n",
            ", on SAM: i i \n",
            "\n",
            ". ! do would , fox could i . i GUY: ham in do SAM: ? with box eggs ! do ! \n",
            "could a , i \n",
            "ham \n",
            "with not would \n",
            "GUY: , GUY: sam-i-am \n",
            "\n",
            "would \n",
            "the , a SAM: SAM: say dark not could say them anywhere not sam-i-am GUY: . \n",
            "and . eggs thank do say in in SAM: like sam-i-am \n",
            "tree GUY: \n",
            "them not or are . a , GUY: ,\n"
          ]
        }
      ],
      "source": [
        "print(postprocess(sample_sequence(unigram_model, ())))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf82e1e7",
      "metadata": {
        "id": "bf82e1e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49c9f1bd-0c79-4e2d-a1e3-81b339f3b708"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SAM: sam ! \n",
            "SAM: try them , so good , will let me be ! \n",
            "GUY: and i would eat them here or there . \n",
            "GUY: i do not eat them anywhere . \n",
            "GUY: and ham . \n",
            "GUY: i do not , would you will eat them ! \n",
            "SAM: could not with a train ! \n",
            "GUY: i would not like them in the dark . \n",
            "GUY: i do not , sam-i-am . \n",
            "SAM: would you , sam-i-am . \n",
            "SAM: eat them on a train ! \n",
            "GUY: and ham !\n"
          ]
        }
      ],
      "source": [
        "print(postprocess(sample_sequence(bigram_model, (\"<s>\",))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66321379",
      "metadata": {
        "id": "66321379",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3de27799-9170-44c3-d63b-01c3b761b6a4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SAM: you may , i will not eat green eggs and ham ? \n",
            "GUY: i do not like green eggs and ham . \n",
            "GUY: i do not like them anywhere ! \n",
            "SAM: say ! \n",
            "GUY: and i will eat them in a house . \n",
            "GUY: that sam-i-am ! \n",
            "GUY: not in a tree ! \n",
            "GUY: i do not like them in a tree . \n",
            "GUY: not in a box . \n",
            "GUY: not in the dark . \n",
            "GUY: not in the dark ! \n",
            "SAM: would you , in a car !\n"
          ]
        }
      ],
      "source": [
        "print(postprocess(sample_sequence(trigram_model, (\"<s>\", \"SAM:\"))))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c169b5c7",
      "metadata": {
        "id": "c169b5c7"
      },
      "source": [
        "# Evaluating text according to an $n$-gram model\n",
        "\n",
        "## The probability metric\n",
        "\n",
        "The main point of a language model is to assign probabilities (or similar scores) to texts. For $n$-gram models, that's done according to Equation (1) at the start of the lab. Let's implement that. We define a function `probability` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the probability of the token sequence  according to the model. It merely multiplies all of the $n$-gram probabilities for all of the $n$-grams in the token sequence.\n",
        "\n",
        "> Throughout this lab, we ignore the scores of the first $n-1$ tokens as our $n$-gram model cannot score them due to the lack of context. In the next lab you will see how to solve this issue in practice."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5a13148",
      "metadata": {
        "id": "d5a13148"
      },
      "outputs": [],
      "source": [
        "def probability(tokens, model, n):\n",
        "    \"\"\"Returns the probability of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = 1.0\n",
        "    context = tokens[0:n-1]\n",
        "    # Ignores the scores of the first n-1 tokens\n",
        "    for token in tokens[n-1:]:\n",
        "        prob = model[tuple(context)][token]\n",
        "        score *= prob\n",
        "        context = (context + [token])[1:]\n",
        "    return score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fb1fb4f",
      "metadata": {
        "id": "9fb1fb4f"
      },
      "source": [
        "We test it on the test text that we held out from the training text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b66ec3b",
      "metadata": {
        "id": "6b66ec3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b52bf871-6381-47e0-f5da-7208996de629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test probability - unigram: 6.404571e-154\n",
            "Test probability -  bigram: 9.147262e-44\n",
            "Test probability - trigram: 0.000000e+00\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test probability - unigram: {probability(test_tokens, unigram_model, 1):6e}\\n\"\n",
        "      f\"Test probability -  bigram: {probability(test_tokens, bigram_model, 2):6e}\\n\"\n",
        "      f\"Test probability - trigram: {probability(test_tokens, trigram_model, 3):6e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6b9064a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b6b9064a"
      },
      "source": [
        "## The negative log probability metric\n",
        "\n",
        "Yikes, those probabilities are _really small_. Multiplying all those small numbers is likely to lead to underflow. \n",
        "\n",
        "To solve the underflow problem, we'll do our usual trick of using negative log probabilities \n",
        "\n",
        "$$ - \\log_2 \\left(\\prod_{i=1}^N \\Prob (w_i \\given w_{i-n+1}, \\cdots, w_{i-1})\\right)$$\n",
        "\n",
        "instead of probabilities.\n",
        "\n",
        "Define a function `neglogprob` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the negative log probability of the token sequence according to the model, calculating it in such a way as to avoid underflow. (You'll want to simplify the formula above before implementing it.)\n",
        "\n",
        "> Be careful when confronting zero probabilities. Taking `-math.log2(0)` raises a \"Math domain error\". Instead, you should use `math.inf` (Python's representation of infinity) as the value for the negative log of zero. This accords with our understanding that an impossible event would require infinite bits to specify.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: neglogprob\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41706ee",
      "metadata": {
        "id": "a41706ee"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def neglogprob(tokens, model, n):\n",
        "    \"\"\"Returns the negative log probability of a sequence of `tokens`\n",
        "       according to an `n`-gram `model`\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    context = tokens[0:n-1]\n",
        "    # Ignores the scores of the first n-1 tokens\n",
        "    for token in tokens[n-1:]:\n",
        "        prob = model[tuple(context)][token]\n",
        "        if(prob == 0):\n",
        "            return math.inf\n",
        "        score -= math.log2(prob)\n",
        "        context = (context + [token])[1:]\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "077fafc5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "077fafc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "ea450e8d-1d2a-49f7-d62d-107d52e22fc6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "grader.check(\"neglogprob\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50204f6e",
      "metadata": {
        "id": "50204f6e"
      },
      "source": [
        "We compute the negative log probabilities of the test text using the different models and report on them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2df5d2d",
      "metadata": {
        "id": "d2df5d2d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c487b6cb-411d-4c2b-9de5-f6269fac2890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test neglogprob - unigram: 508.897825\n",
            "Test neglogprob -  bigram: 142.971496\n",
            "Test neglogprob - trigram:    inf\n"
          ]
        }
      ],
      "source": [
        "unigram_test_nlp = neglogprob(test_tokens, unigram_model, 1)\n",
        "bigram_test_nlp = neglogprob(test_tokens, bigram_model, 2)\n",
        "trigram_test_nlp = neglogprob(test_tokens, trigram_model, 3)\n",
        "\n",
        "print(f\"Test neglogprob - unigram: {unigram_test_nlp:6f}\\n\"\n",
        "      f\"Test neglogprob -  bigram: {bigram_test_nlp:6f}\\n\"\n",
        "      f\"Test neglogprob - trigram: {trigram_test_nlp:6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d473ac4",
      "metadata": {
        "id": "1d473ac4"
      },
      "source": [
        "There, those numbers seem more manageable. We can even convert the neglogprobs back into probabilities as a sanity check."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cd31d13",
      "metadata": {
        "id": "2cd31d13",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68ac356d-f166-4f3f-c9db-41eee3b8baa8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test probability - unigram: 6.404571e-154\n",
            "Test probability -  bigram: 9.147262e-44\n",
            "Test probability - trigram: 0.000000e+00\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test probability - unigram: {2 ** (-unigram_test_nlp):6e}\\n\"\n",
        "      f\"Test probability -  bigram: {2 ** (-bigram_test_nlp):6e}\\n\"\n",
        "      f\"Test probability - trigram: {2 ** (-trigram_test_nlp):6e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fb464f0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "3fb464f0"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Why does the bigram model assign a lower neglogprob (that is, a higher probability) to the test text than the unigram model? Why does the trigram model assign a higher neglogprob (lower probability) to the test text than the other models?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_ordering\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "200541c1",
      "metadata": {
        "id": "200541c1"
      },
      "source": [
        "As n grows, there are more and more possible perumations of the vocabulary and the probability to get each permuration at a constant size of texts gets smaller with n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8998e652",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8998e652"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "## The perplexity metric\n",
        "\n",
        "Another metric that is commonly used is _perplexity_. Jurafsky and Martin give a definition for perplexity as the \"inverse probability of the test set normalized by the number of words\":\n",
        "\n",
        "$$ PP(x_1, x_2, \\ldots, x_N) = \n",
        "     \\sqrt[N]{\\frac{1}{\\prod_{i=1}^N \\Prob (x_i \\given x_{i-n+1}, \\cdots, x_{i-1})}}\n",
        "$$\n",
        "\n",
        "Define a function `perplexity` that takes a token sequence and an $n$-gram model (and the $n$ of the model as well) and returns the perplexity of the token sequence according to the model, calculating it in such a way as to avoid underflow. (By now you're smart enough to realize that you'll want to carry out most of that calculation inside a $\\log$.)\n",
        "\n",
        "> Remember that we ignored the scores of the first n-1 tokens, what should the number of words `N` be?\n",
        "\n",
        "> Hint: Use the `neglogprob` function you defined above.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: perplexity\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e369baf0",
      "metadata": {
        "id": "e369baf0"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "def perplexity(tokens, model, n):\n",
        "    \"\"\"Returns the perplexity of a sequence of `tokens` according to an\n",
        "       `n`-gram `model`\n",
        "    \"\"\"\n",
        "    N = len(tokens) - (n-1)\n",
        "    return 2**(neglogprob(tokens, model, n)/N)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1306c196",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1306c196",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "c2396f4f-c8c6-494f-f4a8-7f2c24712dfb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "grader.check(\"perplexity\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b443e9d8",
      "metadata": {
        "id": "b443e9d8"
      },
      "source": [
        "We can look at the perplexity of the test sample according to each of the models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "febff32f",
      "metadata": {
        "id": "febff32f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c125b2ff-c83a-4151-bd25-4efc2d84da20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 31.761\n",
            "Test perplexity -  bigram: 2.668\n",
            "Test perplexity - trigram: inf\n"
          ]
        }
      ],
      "source": [
        "print(f\"Test perplexity - unigram: {perplexity(test_tokens, unigram_model, 1):.3f}\\n\"\n",
        "      f\"Test perplexity -  bigram: {perplexity(test_tokens, bigram_model, 2):.3f}\\n\"\n",
        "      f\"Test perplexity - trigram: {perplexity(test_tokens, trigram_model, 3):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f63cd2e8",
      "metadata": {
        "id": "f63cd2e8"
      },
      "source": [
        "A perplexity value of $P$ can be interpreted as a measure of a model's average uncertainty in selecting each word equivalent to selecting among $P$ equiprobable words on average. The bigram model gives a perplexity of less than 3, indicating that at each word in the sentence, the model is acting as if selecting among (slightly less than) three equiprobable words.\n",
        "\n",
        "For comparison, state of the art $n$-gram language models for more representative English text achieve perplexities of about 250."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9cab701",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b9cab701"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Smoothing $n$-gram language models\n",
        "\n",
        "> **This section is more open-ended in nature.**\n",
        "\n",
        "The models we've been using have lots of zero-probability $n$-grams. Essentially any $n$-gram that doesn't appear in the training text is imputed a probability of zero, which means that any sentence that contains that $n$-gram will also be given a zero probability. Clearly this is not an accurate estimate.\n",
        "\n",
        "There are many ways to _smooth_ $n$-gram models, just as you smoothed classification models in earlier labs. The simplest is probably add-$\\delta$ smoothing. \n",
        "\n",
        "$$ \\Prob(w_i \\given w_1 \\ldots w_{i-1})\n",
        "  \\approx \\frac{\\cnt{w_1, w_2, \\ldots, w_n} + \\delta}{\\cnt{w_1, w_2, \\ldots, w_{n-1}} + \\delta \\cdot |V|}\n",
        "$$\n",
        "\n",
        "Another useful method is to interpolate multiple $n$-gram models, for instance, estimating probabilities as an interpolation of trigram, bigram, and unigram models.\n",
        "\n",
        "$$ \\Prob(w_i \\given w_1 \\ldots w_{i-1}) \\approx\n",
        "     \\lambda_2 \\Prob(w_i \\given w_{i-2}, w_{i-1}) \n",
        "     + \\lambda_1 \\Prob(w_i \\given w_{i-1}) \n",
        "     + (1 - \\lambda_1 - \\lambda_2) \\Prob(w_i)\n",
        "$$\n",
        "\n",
        "Finally, a method called _backoff_ uses higher-order $n$-gram probabilities where available, \"backing off\" to lower order where necessary.\n",
        "\n",
        "$$\n",
        "\\Prob(w_i \\given w_1 \\ldots w_{i-1}) \\approx \\begin{cases}\n",
        "    \\Prob(w_i \\given w_{i-2}, w_{i-1}) & \\mbox{if $\\Prob(w_i \\given w_{i-2}, w_{i-1}) > 0$}\\\\\n",
        "    \\Prob(w_i \\given w_{i-1})          & \\mbox{if $\\Prob(w_i \\given w_{i-2}, w_{i-1}) = 0$ and $\\Prob(w_i \\given w_{i-1}) > 0$}\\\\\n",
        "    \\Prob(w_i)                         & \\mbox{otherwise}\n",
        "  \\end{cases}\n",
        "$$\n",
        "\n",
        "Define a function `ngram_model_smoothed`, like the `ngram_model` function from above, but implementing one of these smoothing methods. Compare its perplexity on some sample text to the unsmoothed model. \n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_smoothed_model\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ccd6bc6",
      "metadata": {
        "id": "1ccd6bc6"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "#TODO\n",
        "Place your definition of `ngram_model_smoothed` and whatever other testing \n",
        "of it you'd like to do in this and subsequent cells.\n",
        "\"\"\"\n",
        "def dot_product_python(A,B):\n",
        "    return sum(a * b for a, b in zip(A, B))\n",
        "\n",
        "def ngram_model_smoothed(vocabulary,train_tokens,factors):\n",
        "    \"\"\"Returns an n-gram probability model calculated by interpolating multiple n-gram models.\n",
        "    factors is a list of interpolation factors between each sub-model required. may or may not sum to 1. (will be normalized)\n",
        "    the \"n\" of the model is calculated by the length of the factors.\n",
        "    \"\"\"\n",
        "    for factor in factors:\n",
        "        if factor < 0:\n",
        "            raise ValueError(\"Non-Negative factors are not allowed\")\n",
        "    sum_factors = sum(factors)\n",
        "    if(sum_factors == 0):\n",
        "        raise ValueError(\"Factors musn't sum to 0\")\n",
        "    factors = [factor/sum_factors for factor in factors]\n",
        "    N = len(factors)\n",
        "    sub_models = {}\n",
        "    for n in range(1,N+1):\n",
        "        sub_models[n] = ngram_model(ngram_counts(vocabulary,train_tokens,n))\n",
        "\n",
        "    ngram_probabilities = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "    for context,targets in sub_models[N].items():\n",
        "        for target,target_count in targets.items():\n",
        "            sub_models_probabilities = [sub_models[n][tuple(list(context)[-n:]) if n>1 else ()][target] for n in range(1,N+1)]\n",
        "            ngram_probabilities[context][target] = dot_product_python(sub_models_probabilities,factors)\n",
        "    return ngram_probabilities  \n",
        "#bad?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's define an interpolation model\n",
        "smoothed_bigrams = ngram_model_smoothed(vocabulary,train_tokens,factors=[1,2])"
      ],
      "metadata": {
        "id": "XS6DRwJhQwj5"
      },
      "id": "XS6DRwJhQwj5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$ \\Prob(eggs \\given eat) \\approx\n",
        "       \\lambda_1 \\Prob(eggs \\given eat) \n",
        "     + (1 - \\lambda_1) \\Prob(eggs)\n",
        "$$\n"
      ],
      "metadata": {
        "id": "jS7LkaczPvKI"
      },
      "id": "jS7LkaczPvKI"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check\n",
        "smoothed_bigrams_eat_eggs = (2/3)*bigram_model[(\"eat\",)][\"eggs\"] + (1/3)*unigram_model[()][\"eggs\"]\n",
        "smoothed_model_eat_eggs = smoothed_bigrams[(\"eat\",)][\"eggs\"]\n",
        "\n",
        "print(f\"Test smoothed - bigrams, eat_eggs: {smoothed_bigrams_eat_eggs}\\n\"\n",
        "      f\"Test smoothed bigram model - eat_eggs: {smoothed_model_eat_eggs}\\n\"\n",
        "      f\"delta: smoothed bigram model - eat_eggs: {(smoothed_bigrams_eat_eggs - smoothed_model_eat_eggs)}\"\n",
        ")"
      ],
      "metadata": {
        "id": "yUdX239nRpoG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "896a7c5e-4ef3-4a45-b10a-ca68949de3e9"
      },
      "id": "yUdX239nRpoG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test smoothed - bigrams, eat_eggs: 0.002328966521106259\n",
            "Test smoothed bigram model - eat_eggs: 0.002328966521106259\n",
            "delta: smoothed bigram model - eat_eggs: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "one = 1\n",
        "two = 4\n",
        "smoothed_bigrams = ngram_model_smoothed(vocabulary,train_tokens,factors=[one,two])\n",
        "smoothed_bigrams_eat_eggs = (two/(one+two))*bigram_model[(\"eat\",)][\"green\"] + (one/(one+two))*unigram_model[()][\"green\"]\n",
        "smoothed_model_eat_eggs = smoothed_bigrams[(\"eat\",)][\"green\"]\n",
        "\n",
        "print(f\"Test smoothed - bigrams, eat_eggs: {smoothed_bigrams_eat_eggs}\\n\"\n",
        "      f\"Test smoothed bigram model - eat_eggs: {smoothed_model_eat_eggs}\\n\"\n",
        "      f\"delta: smoothed bigram model - eat_eggs: {(smoothed_bigrams_eat_eggs - smoothed_model_eat_eggs)}\"\n",
        ")"
      ],
      "metadata": {
        "id": "jRA-RwnSTh7P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea009548-c5d8-434c-f5a1-8f72e213b5a4"
      },
      "id": "jRA-RwnSTh7P",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test smoothed - bigrams, eat_eggs: 0.07412465263993649\n",
            "Test smoothed bigram model - eat_eggs: 0.07412465263993649\n",
            "delta: smoothed bigram model - eat_eggs: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n=2\n",
        "perp_uni = perplexity(test_tokens, unigram_model, 1)\n",
        "perp_bi = perplexity(test_tokens, bigram_model, 2)\n",
        "print(f\"Test perplexity - unigram: {perp_uni:.3f}\\n\")\n",
        "print(f\"Test perplexity - smoothed_bigrams\")\n",
        "print(\"[unigram_weight,bigram_weight]:\\t perplexity\")\n",
        "for i in range(50+1):\n",
        "    lam1 = 1*i/50 + 0*i/50\n",
        "    factors = [1-lam1,lam1]\n",
        "    smoothed_model = ngram_model_smoothed(vocabulary,train_tokens,factors=factors)\n",
        "    print(f\"{['{0:.2f}'.format(elem) for elem in factors ]}:\\t\\t {perplexity(test_tokens, smoothed_model, n):.3f}\")\n",
        "print(f\"\\nTest perplexity -  bigram: {perp_bi:.3f}\")"
      ],
      "metadata": {
        "id": "ooTXpmoBhLJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de488bbd-8171-4318-b2dc-e8c50dfb25ba"
      },
      "id": "ooTXpmoBhLJ5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test perplexity - unigram: 31.761\n",
            "\n",
            "Test perplexity - smoothed_bigrams\n",
            "[unigram_weight,bigram_weight]:\t perplexity\n",
            "['1.00', '0.00']:\t\t 32.160\n",
            "['0.98', '0.02']:\t\t 24.234\n",
            "['0.96', '0.04']:\t\t 20.059\n",
            "['0.94', '0.06']:\t\t 17.266\n",
            "['0.92', '0.08']:\t\t 15.222\n",
            "['0.90', '0.10']:\t\t 13.646\n",
            "['0.88', '0.12']:\t\t 12.386\n",
            "['0.86', '0.14']:\t\t 11.353\n",
            "['0.84', '0.16']:\t\t 10.489\n",
            "['0.82', '0.18']:\t\t 9.753\n",
            "['0.80', '0.20']:\t\t 9.119\n",
            "['0.78', '0.22']:\t\t 8.566\n",
            "['0.76', '0.24']:\t\t 8.079\n",
            "['0.74', '0.26']:\t\t 7.647\n",
            "['0.72', '0.28']:\t\t 7.260\n",
            "['0.70', '0.30']:\t\t 6.913\n",
            "['0.68', '0.32']:\t\t 6.599\n",
            "['0.66', '0.34']:\t\t 6.313\n",
            "['0.64', '0.36']:\t\t 6.051\n",
            "['0.62', '0.38']:\t\t 5.812\n",
            "['0.60', '0.40']:\t\t 5.591\n",
            "['0.58', '0.42']:\t\t 5.387\n",
            "['0.56', '0.44']:\t\t 5.198\n",
            "['0.54', '0.46']:\t\t 5.023\n",
            "['0.52', '0.48']:\t\t 4.859\n",
            "['0.50', '0.50']:\t\t 4.706\n",
            "['0.48', '0.52']:\t\t 4.563\n",
            "['0.46', '0.54']:\t\t 4.428\n",
            "['0.44', '0.56']:\t\t 4.302\n",
            "['0.42', '0.58']:\t\t 4.183\n",
            "['0.40', '0.60']:\t\t 4.070\n",
            "['0.38', '0.62']:\t\t 3.964\n",
            "['0.36', '0.64']:\t\t 3.863\n",
            "['0.34', '0.66']:\t\t 3.767\n",
            "['0.32', '0.68']:\t\t 3.677\n",
            "['0.30', '0.70']:\t\t 3.591\n",
            "['0.28', '0.72']:\t\t 3.508\n",
            "['0.26', '0.74']:\t\t 3.430\n",
            "['0.24', '0.76']:\t\t 3.355\n",
            "['0.22', '0.78']:\t\t 3.284\n",
            "['0.20', '0.80']:\t\t 3.216\n",
            "['0.18', '0.82']:\t\t 3.150\n",
            "['0.16', '0.84']:\t\t 3.088\n",
            "['0.14', '0.86']:\t\t 3.028\n",
            "['0.12', '0.88']:\t\t 2.970\n",
            "['0.10', '0.90']:\t\t 2.915\n",
            "['0.08', '0.92']:\t\t 2.861\n",
            "['0.06', '0.94']:\t\t 2.810\n",
            "['0.04', '0.96']:\t\t 2.761\n",
            "['0.02', '0.98']:\t\t 2.713\n",
            "['0.00', '1.00']:\t\t 2.668\n",
            "\n",
            "Test perplexity -  bigram: 2.668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seem that the interpolation model basically iterpolates the perplexity contiously. this result is somewhat expected from an interpolation model."
      ],
      "metadata": {
        "id": "LWKwNhW7aqVT"
      },
      "id": "LWKwNhW7aqVT"
    },
    {
      "cell_type": "markdown",
      "id": "04c71003",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "04c71003"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2570a60",
      "metadata": {
        "id": "f2570a60"
      },
      "source": [
        "*  The lab was ok with time\n",
        "*  The reading was appropriate\n",
        "*  The lab was quite clear but the last section where we had a freeroam\n",
        "*  No."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b49448a5",
      "metadata": {
        "id": "b49448a5"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of Lab 2-1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2fb011ed",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2fb011ed"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1aa6ede2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1aa6ede2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "43277482-f2b8-4c62-bbf4-95c3caa469a6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "all_ngrams:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "neglogprob:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "ngram_counts:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "ngram_model:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "perplexity:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "sample_sequence:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>all_ngrams:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>neglogprob:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>ngram_counts:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>ngram_model:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>perplexity:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>sample_sequence:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "otter-latest",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 2-1: Language modeling with n-grams",
    "vscode": {
      "interpreter": {
        "hash": "4fba83c08fc02185bb2310bd24d0cd81fb04529c933f82aa81c61aab9d5528dc"
      }
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}