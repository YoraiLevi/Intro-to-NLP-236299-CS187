{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "8eb0928c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "id": "8eb0928c",
        "outputId": "a3e4f4d6-53a4-41ac-f74e-f6d2a0ce8e62",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab2-4.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "5dc24d0f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5dc24d0f"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "7a046aa5",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "7a046aa5"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d242b05a",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "tags": [
          "remove_for_latex"
        ],
        "id": "d242b05a"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74af9c88",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "74af9c88"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 2-4 â€“ Sequence labeling with hidden Markov models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f2c78fbc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f2c78fbc"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d47c1ba3",
      "metadata": {
        "id": "d47c1ba3"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "Hidden Markov models (HMM) are a fundamental generative method for sequence labeling NLP tasks such as part-of-speech tagging (as in the present lab) and information extraction (as in the second project segment). In this lab, you'll train, apply, and evaluate some simple sequence labeling algorithms culminating in HMM.\n",
        "\n",
        "To keep things manageable, the dataset you'll use will involve very few word types, only six (plus a special beginning of sentence token), but these are quite ambiguous with regard to part of speech. We'll use the following labels for parts of speech:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "180ab132",
      "metadata": {
        "id": "180ab132"
      },
      "outputs": [],
      "source": [
        "parts_of_speech = [\n",
        "    \"<bos>\", # beginning of sentence marker\n",
        "    \"N\",     # noun\n",
        "    \"V\",     # main verb\n",
        "    \"M\",     # modal verb\n",
        "    \"P\",     # preposition\n",
        "    \"A\",     # adjective\n",
        "    \"R\"      # adverb\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02ca94f0",
      "metadata": {
        "id": "02ca94f0"
      },
      "source": [
        "The vocabulary of word types, along with their possible parts of speech, is given by the following dictionary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "95f14ee8",
      "metadata": {
        "id": "95f14ee8"
      },
      "outputs": [],
      "source": [
        "vocabulary = {\n",
        "    \"<bos>\":   [\"<bos>\"],\n",
        "    \"can\":     [\"N\", \"V\", \"M\"],\n",
        "    \"canned\":  [\"A\", \"V\"],\n",
        "    \"canners\": [\"N\"],\n",
        "    \"fish\":    [\"N\", \"V\"],\n",
        "    \"for\":     [\"P\"],\n",
        "    \"not\":     [\"R\"]\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b91b044",
      "metadata": {
        "id": "3b91b044"
      },
      "source": [
        "Here are a few sentences constructed with these words:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "6cad126d",
      "metadata": {
        "id": "6cad126d"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "    <bos> canners canned fish\n",
        "    <bos> can canners can fish\n",
        "    <bos> fish can not fish\n",
        "    <bos> can fish can fish can\n",
        "    <bos> canners fish fish for can\n",
        "    <bos> canners can fish for fish\n",
        "    <bos> canners fish for fish\n",
        "    <bos> fish can canned fish\n",
        "    <bos> canners can not can canned fish\n",
        "    <bos> fish can can fish for canners\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8701a0dd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "8701a0dd"
      },
      "source": [
        "and the corresponding POS sequences, for the first few sentences. Complete the rest.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: text_pos\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "8b6976e5",
      "metadata": {
        "id": "8b6976e5"
      },
      "outputs": [],
      "source": [
        "#TODO -- Provide part-of-speech sequences in this format for the rest of the sample sentences.\n",
        "text_pos = \"\"\"\n",
        "    <bos> N V N \n",
        "    <bos> M N V N\n",
        "    <bos> N M R V\n",
        "    <bos> M N V N N\n",
        "    <bos> N V N P N\n",
        "    <bos> N M V P N\n",
        "    <bos> N V P N\n",
        "    <bos> N N A N\n",
        "    <bos> N M R V A N\n",
        "    <bos> N M V N P N\n",
        "\"\"\"\n",
        "\n",
        "text_pos =\"\"\"\n",
        "    <bos> N V N\n",
        "    <bos> M N V N\n",
        "    <bos> N M R V\n",
        "    <bos> M N V N N\n",
        "    <bos> N V N P N\n",
        "    <bos> N M V P N\n",
        "    <bos> N V P N\n",
        "    <bos> N V A N\n",
        "    <bos> N M R V A N\n",
        "    <bos> N M V N P N\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "6cff8cfb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "6cff8cfb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "860d798a-9d02-4ba4-c94f-cd412666c591"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "grader.check(\"text_pos\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d61cf4c7",
      "metadata": {
        "id": "d61cf4c7"
      },
      "source": [
        "> Just to make sure we're all on the same page -- and because you'll need the right tagging to calculate some probabilities below -- you can find our intended solution to this question at <https://pastebin.com/raw/Us8UcWbn>. You should check your solution before moving on.\n",
        "\n",
        "We tokenize the sentences and label the tokens with their POS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a69e5318",
      "metadata": {
        "id": "a69e5318"
      },
      "outputs": [],
      "source": [
        "def tokenize(text):\n",
        "  result = []\n",
        "  for line in text.strip().split(\"\\n\"):\n",
        "    result.append([item for item in line.strip().split()])\n",
        "  return result\n",
        "\n",
        "tagged_text = [list(zip(sentence, poses))\n",
        "                 for sentence, poses \n",
        "                   in zip(tokenize(text), tokenize(text_pos))]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a290455",
      "metadata": {
        "id": "8a290455"
      },
      "source": [
        "Here are a couple of examples to indicate what the tagged sentences look like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "66a1fb0f",
      "metadata": {
        "id": "66a1fb0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "1fa9d170-994e-4fde-e7e1-57489ab6de78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('<bos>', '<bos>'), ('canners', 'N'), ('canned', 'V'), ('fish', 'N')]\n",
            "[('<bos>', '<bos>'), ('can', 'M'), ('canners', 'N'), ('can', 'V'), ('fish', 'N')]\n"
          ]
        }
      ],
      "source": [
        "print(tagged_text[0])\n",
        "print(tagged_text[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12b8bdc4",
      "metadata": {
        "id": "12b8bdc4"
      },
      "source": [
        "For reference, here is a table showing the frequency distribution for each word type and each part of speech it can be used as."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "19e200b0",
      "metadata": {
        "id": "19e200b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "e9d1d295-6656-4963-970a-ceae74dcc68c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TYPE     POS    COUNT\n",
            "<bos>    <bos>  10\n",
            "canners  N       7\n",
            "canned   V       1\n",
            "canned   A       2\n",
            "fish     N      13\n",
            "fish     V       4\n",
            "can      M       6\n",
            "can      V       5\n",
            "can      N       2\n",
            "not      R       2\n",
            "for      P       4\n"
          ]
        }
      ],
      "source": [
        "counts = defaultdict(lambda: defaultdict(int))\n",
        "for sentence in tagged_text:\n",
        "  for type, pos in sentence:\n",
        "    counts[type][pos] += 1\n",
        "\n",
        "print(f'{\"TYPE\":8} {\"POS\":6} {\"COUNT\"}')\n",
        "for type, type_counts in counts.items():\n",
        "  for pos, count in type_counts.items():\n",
        "    print(f'{type:8} {pos:6} {count:2}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b37e5110",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b37e5110"
      },
      "source": [
        "# Majority label\n",
        "The first sequence labeling method we'll use is simply to choose for each word the POS label it most frequently occurs as in the training data. The table above provides the required information directly.\n",
        "\n",
        "Choosing the majority label for a word sequence $\\vect{w} = \\langle{w_1, w_2, \\ldots, w_m}\\rangle$ is tantamount to maximizing the probability of the label sequence assuming independence of the label conditioned on the word, that is, selecting the tag sequence $\\vect{t} = \\langle{t_1, t_2, \\ldots, t_m}\\rangle$ given by\n",
        "$$ \\argmax{\\vect{t}} \\prod_{i=1}^m \\Prob(t_i \\given w_i) $$\n",
        "\n",
        "How would the majority label method label the following test sentence (which we've marked with the words' correct (\"gold\") parts of speech)?\n",
        "\n",
        "> &lt;bos&gt;[&lt;bos&gt;] canners[N] can[V] canned[A] fish[N]\n",
        "\n",
        "Give your answer in the next cell in the form of a list of strings for the POS labels.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: example_majority_labeling\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f63d1608",
      "metadata": {
        "id": "f63d1608"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "# \"<bos> canners can canned fish\"\n",
        "example_majority_labeling = [\"<bos>\", \"N\", \"M\", \"A\", \"N\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "9353fe3b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "9353fe3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "a033aa8e-7884-428f-9b32-212b59f76586"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "grader.check(\"example_majority_labeling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "29519eac",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "29519eac"
      },
      "source": [
        "By inspection, what is the accuracy of the majority labeling, given as a proportion of the words in the sentence (including the beginning of sentence token)?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: example_maj_label_accuracy\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "916b63da",
      "metadata": {
        "id": "916b63da"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "example_maj_label_accuracy = 4/5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f686e926",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f686e926",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "aa0a7744-d47d-451b-ed9d-b188f0ddbd4d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "grader.check(\"example_maj_label_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35a97b9c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "35a97b9c"
      },
      "source": [
        "# Majority bigram labeling\n",
        "\n",
        "It may occur to you that what part of speech a word has _depends on its context_. Suppose we relax the assumption that tag probabilities depend only on the word being tagged, and condition them on the previous word as well. (For the first word in the sentence, we'll condition on that fact, by conditioning it on the special start token.) In summary, we'll condition on the bigram that ends at the word being tagged:\n",
        "$$ \\argmax{\\vect{t}} \\prod_{i=1}^m \\Prob(t_i \\given w_{i-1} w_i) $$\n",
        "\n",
        "What is the majority bigram labeling of the test sentence? Again, give your answer in the form of a list of strings for the POS labels.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: example_majority_bigram_labeling\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "counts = defaultdict(lambda: defaultdict(int))\n",
        "prev = []\n",
        "for sentence in tagged_text:\n",
        "    for type, pos in sentence:\n",
        "        prev = prev+[type]\n",
        "        bigram = tuple(prev[-2:])\n",
        "        counts[bigram][pos] += 1\n",
        "    prev = []\n",
        "\n",
        "print(f'{\"TYPE\":22} {\"POS\":6} {\"COUNT\"}')\n",
        "for type, type_counts in counts.items():\n",
        "  for pos, count in type_counts.items():\n",
        "    print(f'{str(type):22} {pos:6} {count:2}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "eBfqyb6zfNX6",
        "outputId": "23c7f948-8103-4e0b-912b-746b0e1268cb"
      },
      "id": "eBfqyb6zfNX6",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TYPE                   POS    COUNT\n",
            "('<bos>',)             <bos>  10\n",
            "('<bos>', 'canners')   N       5\n",
            "('canners', 'canned')  V       1\n",
            "('canned', 'fish')     N       3\n",
            "('<bos>', 'can')       M       2\n",
            "('can', 'canners')     N       1\n",
            "('canners', 'can')     V       1\n",
            "('canners', 'can')     M       2\n",
            "('can', 'fish')        N       4\n",
            "('can', 'fish')        V       1\n",
            "('<bos>', 'fish')      N       3\n",
            "('fish', 'can')        M       2\n",
            "('fish', 'can')        V       2\n",
            "('fish', 'can')        N       1\n",
            "('can', 'not')         R       2\n",
            "('not', 'fish')        V       1\n",
            "('canners', 'fish')    V       2\n",
            "('fish', 'fish')       N       1\n",
            "('fish', 'for')        P       4\n",
            "('for', 'can')         N       1\n",
            "('for', 'fish')        N       2\n",
            "('can', 'canned')      A       2\n",
            "('not', 'can')         V       1\n",
            "('can', 'can')         V       1\n",
            "('for', 'canners')     N       1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ce9fa283",
      "metadata": {
        "id": "ce9fa283"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "# \"<bos> canners can canned fish\"\n",
        "example_majority_bigram_labeling = [\"<bos>\", \"N\", \"M\", \"A\", \"N\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "efdee26f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "efdee26f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "ce69e5a5-5563-4ef0-9354-1333f65761d2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "grader.check(\"example_majority_bigram_labeling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2522498b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2522498b"
      },
      "source": [
        "By inspection, what is the accuracy of the majority bigram labeling, given as a proportion of the words?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: example_maj_bigram_label_accuracy\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "34f7ebd4",
      "metadata": {
        "id": "34f7ebd4"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "example_maj_bigram_label_accuracy = 4/5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "a3d5de75",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a3d5de75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "b59f117c-aeed-42a7-af0f-64bab3aa3ddd"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "grader.check(\"example_maj_bigram_label_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e154fcb3",
      "metadata": {
        "id": "e154fcb3"
      },
      "source": [
        "# Hidden Markov models\n",
        "\n",
        "Now we get to the real point, using an HMM model. Recall that in an HMM model, we assume that the joint tag/word sequence is generated by \n",
        "\n",
        "1. Selecting a tag sequence according to a Markov model whose states correspond to tags and whose transitions from state $t_i$ to $t_j$ are governed by a _transition probability_ $a_{ij} = \\Prob(t_i \\rightarrow t_j)$, and then\n",
        "2. Selecting a word sequence from the tag sequence where for tag $t_i$ we observe word $x_i$ of type $w_j$ governed by an _emission probability_ $b_{i}(w_j) = \\Prob(t_i \\rightarrow w_j)$.\n",
        "\n",
        "> Here, we're using a notation $\\Prob(t_i \\rightarrow t_j)$ to indicate the probability that a word tagged $t_i$ is followed by a word tagged $t_j$ and $\\Prob(t_i \\rightarrow w_j)$ to indicate the probability that a word tagged $t_i$ is the word $w_j$.\n",
        "\n",
        "## Estimating the transition and emission probabilities\n",
        "\n",
        "We estimate these transition and emission probabilities by looking at the empirical probabilities in the training data, counting and perhaps smoothing as usual. That is, for the (unsmoothed) transition probabilities, we estimate\n",
        "$$ a_{ij} \\approx \\frac{\\cnt{t_i \\rightarrow t_j}}{\\sum_k \\cnt{t_i \\rightarrow t_k}} $$\n",
        "and for the emission probabilities\n",
        "$$ b_i(w_j) \\approx \\frac{\\cnt{t_i \\rightarrow w_j}}{\\cnt{t_i}} $$\n",
        "\n",
        "For instance, we note that there are 4 times in the training data where the tag $N$ is followed by the tag $M$, out of the 21 occurrences of the tag $N$. Thus, we estimate the corresponding transition probability $a_{NM} \\approx 4/21$.\n",
        "\n",
        "\n",
        "Similarly, the emission probability $b_M(can)$ for tag $M$ generating the word $can$ is $6/6 = 1$, since every occurrence of the tag $M$ corresponds to the word $can$ in the training data.\n",
        "\n",
        "For your convenience, we've computed and provided full tables for the transition and emission probabilities below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "b18fb602",
      "metadata": {
        "id": "b18fb602",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "8eb814fe-9ec2-4a61-fb65-06ae4d1f40f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transition probabilities: a_ij\n",
            "       <bos>     N     V     M     P     A     R\n",
            "<bos>   0.00  0.80  0.00  0.20  0.00  0.00  0.00\n",
            "N       0.00  0.08  0.46  0.31  0.15  0.00  0.00\n",
            "V       0.00  0.56  0.00  0.00  0.22  0.22  0.00\n",
            "M       0.00  0.33  0.33  0.00  0.00  0.00  0.33\n",
            "P       0.00  1.00  0.00  0.00  0.00  0.00  0.00\n",
            "A       0.00  1.00  0.00  0.00  0.00  0.00  0.00\n",
            "R       0.00  0.00  1.00  0.00  0.00  0.00  0.00\n",
            "\n",
            "Emission probabilities: b_i(w_j)\n",
            "         <bos>     can  canned canners    fish     for     not\n",
            "<bos>     1.00    0.00    0.00    0.00    0.00    0.00    0.00\n",
            "N         0.00    0.09    0.00    0.32    0.59    0.00    0.00\n",
            "V         0.00    0.50    0.10    0.00    0.40    0.00    0.00\n",
            "M         0.00    1.00    0.00    0.00    0.00    0.00    0.00\n",
            "P         0.00    0.00    0.00    0.00    0.00    1.00    0.00\n",
            "A         0.00    0.00    1.00    0.00    0.00    0.00    0.00\n",
            "R         0.00    0.00    0.00    0.00    0.00    0.00    1.00\n"
          ]
        }
      ],
      "source": [
        "# Generate counts\n",
        "bigram_tag_counts = defaultdict(lambda: defaultdict(int))\n",
        "unigram_tag_counts = defaultdict(int)\n",
        "tag_word_counts = defaultdict(lambda: defaultdict(int))\n",
        "tag_counts = defaultdict(int)\n",
        "\n",
        "for sentence in tagged_text:\n",
        "    for (w1, t1), (w2, t2) in list(zip(sentence[:-1], sentence[1:])):\n",
        "        bigram_tag_counts[t1][t2] += 1\n",
        "        unigram_tag_counts[t1] += 1\n",
        "    for w, t in sentence:\n",
        "        tag_word_counts[t][w] += 1\n",
        "        tag_counts[t] += 1\n",
        "\n",
        "# Generate transition and emission probabilities\n",
        "a = defaultdict(lambda: defaultdict(int))\n",
        "b = defaultdict(lambda: defaultdict(int))\n",
        "\n",
        "for t1 in parts_of_speech:\n",
        "    for t2 in parts_of_speech:\n",
        "        a[t1][t2] = bigram_tag_counts[t1][t2] / unigram_tag_counts[t1]\n",
        "    for w1 in vocabulary.keys():\n",
        "        b[t1][w1] = tag_word_counts[t1][w1] / tag_counts[t1]\n",
        "\n",
        "# Print tables of probabilities\n",
        "\n",
        "print(\"Transition probabilities: a_ij\")\n",
        "print(f\"{' ':6}\", end=\"\")\n",
        "for t in parts_of_speech:\n",
        "    print(f\"{t:>6}\", end=\"\")\n",
        "print()\n",
        "for t1 in parts_of_speech:\n",
        "    print(f\"{t1:<6}\", end=\"\")\n",
        "    for t2 in parts_of_speech:\n",
        "        print(f\"{a[t1][t2]:>6.2f}\", end=\"\")\n",
        "    print(\"\")\n",
        " \n",
        "print(\"\\nEmission probabilities: b_i(w_j)\")\n",
        "print(f\"{' ':6}\", end=\"\")\n",
        "for w in vocabulary.keys():\n",
        "    print(f\"{w:>8}\", end=\"\")\n",
        "print()\n",
        "for t in parts_of_speech:\n",
        "    print(f\"{t:<6}\", end=\"\")\n",
        "    for w in vocabulary.keys():\n",
        "        print(f\"{b[t][w]:>8.2f}\", end=\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e78d5ae9",
      "metadata": {
        "id": "e78d5ae9"
      },
      "source": [
        "## An example HMM trellis\n",
        "\n",
        "<img src=\"https://github.com/nlp-course/data/raw/master/Resources/hmm-figure.png\" width=\"75%\" align=right />\n",
        "\n",
        "Now consider the HMM generating the example sentence \"canners can canned fish\". The figure at right contains the _trellis_ for the sentence. The horizontal axis corresponds to the words in the sentence, one at a time. The vertical axis corresponds to the states of the HMM (that is, the parts of speech). The gray arrows that connect a tag on the left to a tag on the right correspond to the transition probabilities. The red arrows that connect a tag to a word directly below correspond to the emission probabilities. \n",
        "\n",
        "For convenient reference, we've labeled the rows with letters and the columns with numbers (in teal!) so that particular nodes can be referred to as, for example, B1.\n",
        "\n",
        "(The red lines are intended to go from a state node to the word node directly below it; for instance, the red line immediately below B1 goes to the word node F1 at the bottom of the figure. Some of those lines are depicted with a crossbar to indicate that they are running \"underneath\" other graphic objects in the figure.)\n",
        "\n",
        "We've highlighted two paths through the trellis from the beginning to the end of the sentence, corresponding to different taggings of the sentence:\n",
        "\n",
        "1. A0-B1-C2-E3-B4\n",
        "2. A0-B1-D2-C3-B4\n",
        "\n",
        "Answer the following questions about this trellis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "731ba26f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "731ba26f"
      },
      "source": [
        "What is the path through the trellis corresponding to the _majority bigram labeling_ that you determined above? Give your answer as a string, like the path examples in the previous cell.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: majority_path\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "1ace7dde",
      "metadata": {
        "id": "1ace7dde"
      },
      "outputs": [],
      "source": [
        "#[\"<bos>\", \"N\", \"M\", \"A\", \"N\"]\n",
        "majority_path = \"A0-B1-D2-E3-B4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "75161f43",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "75161f43",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "f5fac7cc-c1f7-47f9-a40a-7a9dd4ce8f3f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "grader.check(\"majority_path\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d689cc59",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d689cc59"
      },
      "source": [
        "The probability of a path is just the product of the probabilities of all the transitions along the path and all the emissions from nodes in the path to observed words. Use the tables above to calculate the probability of the first highlighted path (A0-B1-C2-E3-B4) by multiplying together the appropriate probabilities. Don't forget the emission probabilities, corresponding to the edges A0-F0, B1-F1, C2-F2, E3-F3, and B4-F4.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: highlight1_path_probability\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "e146179b",
      "metadata": {
        "id": "e146179b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "58784bde-1ab3-4acd-80c1-d7e215d80c0b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0026002943999999997"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ],
      "source": [
        "#TODO\n",
        "# A0-B1-C2-E3-B4\n",
        "transitions = (0.8)*(0.36)*(0.12)*(1)\n",
        "emisions = (1)*(0.3)*(0.44)*(1)*(0.57)\n",
        "highlight1_path_probability = transitions * emisions\n",
        "highlight1_path_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "f8dd7fed",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f8dd7fed",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "036eda86-eea1-46b0-b739-5da781b816a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "grader.check(\"highlight1_path_probability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9c9935c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a9c9935c"
      },
      "source": [
        "Do the same for the second highlighted path (A0-B1-D2-C3-B4).\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: highlight2_path_probability\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "5d67d112",
      "metadata": {
        "id": "5d67d112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "a9ec11fa-6066-4a07-817f-42da1c5e03b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0009711789120000001"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "#TODO\n",
        "transitions = (0.8) * (0.29) * (0.33) * (0.62)\n",
        "emisions = (1) * (0.3) * (1) * (0.11) * (0.62)\n",
        "highlight2_path_probability = transitions * emisions\n",
        "highlight2_path_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "5dc1efc7",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "5dc1efc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "63aca4e2-d96b-4461-be9b-a8f6fb59883f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "grader.check(\"highlight2_path_probability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0a1ba40",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e0a1ba40"
      },
      "source": [
        "These two paths turn out to be the two paths through the trellis with the highest probabilities. (You'll have to trust us.) Based on that fact, which tagging has the highest probability according to this HMM? Give your solution in the same format as you did for `example_majority_labeling` above.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: example_highest_labeling\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "ac43f3e4",
      "metadata": {
        "id": "ac43f3e4"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "example_highest_labeling = [\"<bos>\", \"N\", \"V\", \"A\", \"N\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "702f5aea",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "702f5aea",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "5636dd86-3813-4e8c-8670-7a896ed04693"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "grader.check(\"example_highest_labeling\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ea267fb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "7ea267fb"
      },
      "source": [
        "By inspection, what is the accuracy of the highest probability HMM labeling, given as a proportion of the words?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: example_highest_label_accuracy\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "49065983",
      "metadata": {
        "id": "49065983"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "example_highest_label_accuracy = 5/5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "id": "4bbafeab",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4bbafeab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "8b5be26d-5fbf-448d-cde1-32788378494c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "grader.check(\"example_highest_label_accuracy\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac007555",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ac007555"
      },
      "source": [
        "Now, recall the majority bigram labeling, and the path for it that you developed above. What is the probability of that path according to the HMM?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: majority_path_probability\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "97ff4591",
      "metadata": {
        "id": "97ff4591"
      },
      "outputs": [],
      "source": [
        "transitions = (0.8) * (0.29) * (0) * (1)\n",
        "emisions = (1) * (0.3) * (1) * (1) * (0.57)\n",
        "majority_path_probability = transitions*emisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "a1d792f3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a1d792f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "c232e615-9d7e-4e44-bf97-a8e672a24172"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "grader.check(\"majority_path_probability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4b6ed56",
      "metadata": {
        "id": "b4b6ed56"
      },
      "source": [
        "## Calculating the highest probability tagging - The Viterbi algorithm\n",
        "\n",
        "Above, we merely asserted that the two highlighted paths are the two most probable, so that it was a simple matter to find the highest probability tagging by just comparing the probabilities of those two. But in general there can be a huge number of paths through a trellis such as this."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32bd469a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "32bd469a"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** If there are $N$ tags and a sentence of length $M$, how many paths through the HMM trellis will there be (using big-$O$ notation)?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_how_many_paths\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed20e26b",
      "metadata": {
        "id": "ed20e26b"
      },
      "source": [
        "$O(N^M)$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d8ffc91",
      "metadata": {
        "id": "0d8ffc91"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "The Viterbi algorithm, named after famed electrical engineer [Andrew Viterbi](https://en.wikipedia.org/wiki/Andrew_Viterbi), is an efficient dynamic programming algorithm for performing this (otherwise impractical) computation. We'll do the first few steps of the Viterbi algorithm for the example here.\n",
        "\n",
        "Given a string of words $\\vect{x} = \\langle w_0, w_1, \\ldots, w_M \\rangle$ and a set of states (tags) $\\vect{q} = \\{q_0, q_1, \\ldots, q_N\\}$, the algorithm works by calculating a series of values $v_i(j)$ where $i$ ranges over the words in the sentence from $1$ to $M$ and $j$ ranges over the tags from $1$ to $N$. For simplicity, we'll assume an extra word and tag at the beginning of the sentence, as above, so $w_0 = \\texttt{<bos>}$ and $q_0 = \\texttt{<bos>}$.\n",
        "\n",
        "The values $v_i(j)$ correspond to the probability of the best (highest probability) path starting in state 0, emitting $w_0, \\ldots, w_i$, and ending in state $q_j$. The definition for $v$ then is:\n",
        "\n",
        "\\begin{align*}\n",
        "  v_0(0) &= 1 \\\\\n",
        "  v_0(j) &= 0  &\\mbox{for $j > 0$} \\\\\n",
        "  v_i(j) &= \\max_{j'=0}^N v_{i-1}(j') \\cdot a_{j' j} \\cdot b_{j}(w_i)\n",
        "         & \\mbox{for $i > 0$}\n",
        "\\end{align*}\n",
        "\n",
        "In addition, we'll want to keep track of which states the best paths go through, so in addition to tracking the best probability with $v_i(j)$, we'll track the immediately preceding state that led to that best path -- the \"back pointer\" -- with $bp_i(j)$:\n",
        "\n",
        "\\begin{align*}\n",
        "  bp_i(j) &= \\operatorname*{argmax}_{j'=0}^N v_{i-1}(j') \\cdot a_{j' j} \\cdot b_{j}(w_i)\n",
        "         & \\mbox{for $i > 0$}\n",
        "\\end{align*}\n",
        "\n",
        "> Notice how similar the back pointer definition is to the Viterbi best path definition. It merely records which state was used in computing the corresponding best path."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2bc59557",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "2bc59557"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** For the sample sentence above (\"canners can canned fish\"), calculate the five \"layers\" of the Viterbi algorithm, that is, $v_0$, $v_1$, $v_2$, $v_3$, and $v_4$, and the corresponding back pointers by filling in the tables below. (We've filled in the $v_0$ column for you already, as per the first two lines in the definition of the Viterbi calculation. No $bp_0$ backpointer is needed (or defined).)\n",
        "\n",
        "> For the back pointer table, you need only provide entries corresponding to cases where $bp_i(j)$ is non-zero.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_viterbi_table\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d414ca69",
      "metadata": {
        "id": "d414ca69"
      },
      "source": [
        "<!--TODO: Highest probability table -->\n",
        "|   | tag         | v_0 &lt;bos&gt; | v_1 canners  | v_2 can                | v_3 canned                   | v_4 fish                   |\n",
        "|---|:-----------:|:---------------:|:------------:|:----------------------:|:----------------------------:|:--------------------------:|\n",
        "| 0 | &lt;bos&gt; |    1            | 0            | 0                      | 0                            | 0                          |\n",
        "| 1 | N           |    0            | 0.8*0.3=0.24 | 0                      | 0                            | 0.012144*1*0.59=0.00716496 |\n",
        "| 2 | V           |    0            | 0.2*0=0      | 0.24*0.46*0.5=0.0552   | 0.0744*0.33*0.1=0.0024552    | 0                          |\n",
        "| 3 | M           |    0            | 0            | 0.24*0.31*1=0.0744     | 0                            | 0                          |\n",
        "| 4 | P           |    0            | 0            | 0                      | 0                            | 0                          |\n",
        "| 5 | A           |    0            | 0            | 0                      | 0.0552*0.22*1=0.012144       | 0                          |\n",
        "| 6 | R           |    0            | 0            | 0                      | 0                            | 0                          |\n",
        "\n",
        "<!--TODO: Backpointers for best path table -->\n",
        "|   | tag         | bp_1 canners                   | bp_2 can | bp_3 canned | bp_4 fish |\n",
        "|---|:-----------:|:------------------------------:|:--------:|:-----------:|:---------:|\n",
        "| 0 | &lt;bos&gt; | 0                              | -        | -           | -         |\n",
        "| 1 | N           | 0                              | 1        | -           | 5         |\n",
        "| 2 | V           | 0                              | 1        | 3           | -         |\n",
        "| 3 | M           | 0                              | -        | -           | -         |\n",
        "| 4 | P           | 0                              | -        | -           | -         |\n",
        "| 5 | A           | 0                              | -        | 2           | -         |\n",
        "| 6 | R           | 0                              | -        | -           | -         |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0d9d11c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a0d9d11c"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "Doing this calculation by hand is painful, but it should make clear what's going on. At each entry $v_i(j)$ in the table, we've calculated the probability of the best path through the trellis from the beginning of the sentence to the current word  $x_i$, starting in the start state and ending in the current state $q_j$. To get the maximum probability of all paths in the trellis for the full sentence ending in any state, we merely look up the maximum value of $v_M(j)$ (recalling that $M$ is the length of the sentence).\n",
        "\n",
        "**Question:** Based on the tables you filled out above, what is the probability of the best path through the trellis for the sample sentence?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: best_path_probability\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "c5c9b798",
      "metadata": {
        "id": "c5c9b798"
      },
      "outputs": [],
      "source": [
        "best_path_probability = 0.00716496"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "35f04a04",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "35f04a04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "18f43c07-1a75-4ac6-c3d4-36ff4277e7dc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "grader.check(\"best_path_probability\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e3a41ef2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e3a41ef2"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Based on the tables you filled out above, what is the best label sequence for the sentence? Provide it as a list of strings for the variable `best_label_sequence`, e.g., `['<bos>','V','A','M','A']`. (No, that's not the actual answer.)\n",
        "\n",
        "> Hint: You'll get this by tracing back the backpointers.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: best_label_sequence\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "412c92c7",
      "metadata": {
        "id": "412c92c7"
      },
      "outputs": [],
      "source": [
        "best_label_sequence = [\"<bos>\", \"N\", \"V\", \"A\", \"N\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "4697de51",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4697de51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "outputId": "95d7c1b5-01d9-4811-f3f5-a63b59950581"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ],
      "source": [
        "grader.check(\"best_label_sequence\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d919c017",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "d919c017"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What is the complexity of filling in all of the entries in the Viterbi table? How does that compare with the complexity of the total number of paths through the trellis that you calculated above?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_how_many_paths_viterbi\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f353d8f7",
      "metadata": {
        "id": "f353d8f7"
      },
      "source": [
        "**Answer**: *For every calculation of the i-th iteration, we need to calculate the max over all the outputs of the (i-1)-th calculation, that is calculating the maximum over N outputs ($O(N)$). That means that each iteration (word) takes $O(N^2)$ steps calculate. For a sequence of $M$ tokens, that complexity is thus $O(M \\cdot N^2)$, which is better than the overall number of possible paths*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1c8cce8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "b1c8cce8"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c9feb30b",
      "metadata": {
        "id": "c9feb30b"
      },
      "source": [
        "_Type your answer here, replacing this text._"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ac259b3",
      "metadata": {
        "id": "9ac259b3"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of lab 2-4"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6e3c5e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "4c6e3c5e"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "a2c65168",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a2c65168",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 630
        },
        "outputId": "13e31f38-d441-46c1-c67e-1203c8deff26"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "best_label_sequence:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "best_path_probability:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "example_highest_label_accuracy:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "example_highest_labeling:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "example_maj_bigram_label_accuracy:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "example_maj_label_accuracy:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "example_majority_bigram_labeling:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "example_majority_labeling:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "highlight1_path_probability:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "highlight2_path_probability:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "majority_path:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "majority_path_probability:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "text_pos:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>best_label_sequence:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>best_path_probability:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>example_highest_label_accuracy:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>example_highest_labeling:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>example_maj_bigram_label_accuracy:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>example_maj_label_accuracy:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>example_majority_bigram_labeling:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>example_majority_labeling:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>highlight1_path_probability:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>highlight2_path_probability:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>majority_path:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>majority_path_probability:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>text_pos:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 2-4: Sequence labeling with hidden Markov models",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}