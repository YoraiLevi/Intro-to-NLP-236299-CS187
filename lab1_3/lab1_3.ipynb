{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b64b1325",
      "metadata": {
        "deletable": false,
        "editable": false,
        "jupyter": {
          "outputs_hidden": true,
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b64b1325",
        "outputId": "cf3e0971-953f-4d4b-be6c-3ee1c1dddec6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab1-3.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "ea1ee6a1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ea1ee6a1"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ]
    },
    {
      "cell_type": "raw",
      "id": "db8b78c1",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "db8b78c1"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0eedd05",
      "metadata": {
        "id": "c0eedd05"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dd4661e",
      "metadata": {
        "tags": [
          "remove_for_latex"
        ],
        "id": "0dd4661e"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 1-3 ‚Äì Naive Bayes classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f675eda",
      "metadata": {
        "id": "0f675eda"
      },
      "source": [
        "In this lab, you'll apply the naive Bayes method to the _Federalist_ papers' authorship attribution problem.\n",
        "\n",
        "After this lab, you should be able to\n",
        "\n",
        "* Derive the basic equations for the naive Bayes classification method;\n",
        "* Estimate the parameters for the naive Bayes model;\n",
        "* Determine where use of the \"log trick\" is indicated, and apply it."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1a79f31",
      "metadata": {
        "id": "d1a79f31"
      },
      "source": [
        "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
        "\n",
        "* [`math.log2`](https://docs.python.org/3.8/library/math.html#math.log2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70707e2a",
      "metadata": {
        "id": "70707e2a"
      },
      "source": [
        "# Preparation ‚Äì Loading packages and data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8721ab97",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "id": "8721ab97"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "import json\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.style.use('tableau-colorblind10')\n",
        "import torch\n",
        "import wget\n",
        "\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "3d16acc9",
      "metadata": {
        "id": "3d16acc9"
      },
      "outputs": [],
      "source": [
        "# Download and read the Federalist data from the json file\n",
        "os.makedirs('data', exist_ok=True)\n",
        "wget.download('https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json', out='data/')\n",
        "with open('data/federalist_data.json', 'r') as fin:\n",
        "    dataset = json.load(fin)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "d9e2687f",
      "metadata": {
        "id": "d9e2687f"
      },
      "outputs": [],
      "source": [
        "# As before, we extract the papers by either of Madison and Hamilton\n",
        "# to serve as training data.\n",
        "training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n",
        "                       dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c66cd3e1",
      "metadata": {
        "id": "c66cd3e1"
      },
      "source": [
        "# The Naive Bayes method reviewed\n",
        "A quick review of the Naive Bayes (NB) method for text classification: In classification tasks, we're given a representation of some text as a vector $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ of feature values, and we'd like to determine which of a set of classes $\\{ y_1, y_2, \\ldots, y_k \\}$ the text should be classified as. \n",
        "\n",
        "> In the case at hand, the Federalist Papers, for a given document, we'll take $\\mathbf{x} = \\langle x_1, x_2, \\ldots, x_m \\rangle$ to be the sequence of words in the document, so each $x_i$ corresponds to a single word token.\n",
        "\n",
        "We might naturally think to choose that class that has the highest probability of being correct, that is, the class $y_i$ that maximizes $Pr(y_i \\mid \\mathbf{x})$.\n",
        "\n",
        "By Bayes rule (this is the \"Bayes\" part in the name \"Naive Bayes\"), \n",
        "\n",
        "\\begin{align*}\n",
        "\\argmax{i} \\Prob(y_i \\given \\vect{x}) \n",
        "&= \\argmax{i} \\frac{\\Prob(\\vect{x} \\given y_i) \\cdot \\Prob(y_i)}{\\Prob(\\vect{x})} \\\\\n",
        "&= \\argmax{i} \\Prob(\\vect{x} \\given y_i) \\cdot \\Prob(y_i)\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad0caa2c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "ad0caa2c"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question**: Why can we drop the denominator in the last step of this derivation?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_denominator\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfdb74af",
      "metadata": {
        "id": "dfdb74af"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "As mentioned in chapter 4.1 in _Speech and Language Processing_ (Jurafsky & Martin), we can drop the denominator since it is both positive and the same among all the possible classes.\n",
        "\n",
        "It is positive since the denominator is a probability value (which is always between 0 and 1), and since a denominator cannot be 0.\n",
        "\n",
        "Additionally, the denominator is the same among all the possible classes, since this probability value does not depend on the class argument.\n",
        "\n",
        "Consequently, dropping the denominator does not influence the maximal argument."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5633c6b",
      "metadata": {
        "id": "c5633c6b"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "We use the following terminology: $\\Prob(y_i)$ is the _prior probability_. $\\Prob(\\vect{x} \\given y_i)$ is the _likelihood_. \n",
        "$\\Prob(y_i \\given \\vect{x})$ is the _posterior probability_.\n",
        "\n",
        "By the chain rule, \n",
        "\n",
        "\\begin{align*}\n",
        "\\Prob(\\vect{x} \\given y_i) &= \\Prob(x_1, \\ldots, x_m \\given y_i) \\\\\n",
        "&= \\Prob(x_1 \\given y_i) \\cdot \\Prob(x_2, \\ldots, x_m \\given x_1, y_i) \\\\\n",
        "&= \\Prob(x_1 \\given y_i) \\cdot \\Prob(x_2 \\given x_1, y_i) \\cdot \\Prob(x_3, \\ldots,\n",
        "x_m \\given x_1, x_2, y_i) \\\\\n",
        "\\cdots &= \\prod_{j=1}^m \\Prob(x_j \\given x_1, \\ldots, x_{j-1}, y_i)\n",
        "\\end{align*}\n",
        "\n",
        "We further assume that each feature $x_j$ is independent of all the others given the class. (That's the \"naive\" part.) So \n",
        "\n",
        "$$\n",
        "\\Prob(x_j \\given x_1, \\ldots, x_{j-1}, y_i) \\approx \\Prob(x_j \\given y_i)\n",
        "$$\n",
        "\n",
        "Using this approximation, we'll calculate instead the class as per the following maximization:\n",
        "\n",
        "$$\n",
        "\\argmax{i} \\Prob(y_i \\given \\vect{x}) \\approx \\argmax{i} \\Prob(y_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given y_i)\n",
        "$$\n",
        "\n",
        "> This independence assumption, in the text case, amounts to ignoring the order and even the cooccurence of words in a document, a quite aggressive and unrealistic independence assumption indeed.\n",
        "\n",
        "All we need, then, for the Naive Bayes classification method is values for $\\Prob(y_i)$ and $\\Prob(x_j \\given y_i)$ for each feature $x_j$ and each class $y_i$. These constitute the parameters of the model, which we will learn from a training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f1c94d5",
      "metadata": {
        "id": "3f1c94d5"
      },
      "source": [
        "# Naive Bayes for the Federalist papers\n",
        "\n",
        "In applying Naive Bayes to an example in the Federalist dataset, we'll take the $x_j$ to be the _tokens in the example_. To make the calculations easier, in this lab, we won't use _all_ of the tokens, just the tokens of the four word types we've been attending to, but in an actual application of NB, we'd use (essentially) all of the word types. As a reminder,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "44147732",
      "metadata": {
        "id": "44147732"
      },
      "outputs": [],
      "source": [
        "keywords = ['on', 'upon', 'there', 'whilst']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae458498",
      "metadata": {
        "id": "ae458498"
      },
      "source": [
        "and the two class labels are"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "16856cdc",
      "metadata": {
        "id": "16856cdc"
      },
      "outputs": [],
      "source": [
        "classes = ['Hamilton', 'Madison']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d8c1df9",
      "metadata": {
        "id": "5d8c1df9"
      },
      "source": [
        "> To clarify, we still treat the input as a bag-of-words representation, but the vocabulary is now limited to four word types."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0a989bce",
      "metadata": {
        "id": "0a989bce"
      },
      "source": [
        "### Estimating the prior probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a40d822d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a40d822d"
      },
      "source": [
        "Let's start with the prior probabilities $\\Prob(y_i)$. In our case, there are only two class labels, for Hamilton and Madison. We estimate the probability of a class $y_i$ by simply counting the proportion of examples that are labeled with that class. (This estimate is the *sample probability*, which is also referred to as the *maximum likelihood estimate* for reasons we'll skip for the moment.) That is, we estimate \n",
        "\n",
        "$$ \\Prob(y_i) \\approx \\frac{\\cnt{y_i}}{N} $$\n",
        "\n",
        "where $N$ is the number of training examples, and $\\cnt{y_i}$ is the number of training examples of class $y_i$.\n",
        "\n",
        "In the cell below, write code to count how many of the training examples are labeled with Hamilton and how many are labeled with Madison. Use these to provide estimates of the Hamilton and Madison prior probabilities.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: priors\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "dd3d66e5",
      "metadata": {
        "id": "dd3d66e5"
      },
      "outputs": [],
      "source": [
        "#TODO - Calculate the prior probabilities for Madison and Hamilton as floats.\n",
        "training_Madison = list(filter(lambda ex: ex['authors'] == 'Madison', training))\n",
        "training_Hamilton = list(filter(lambda ex: ex['authors'] == 'Hamilton', training))\n",
        "\n",
        "prior_madison = len(training_Madison) / len(training)\n",
        "prior_hamilton = len(training_Hamilton) / len(training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "62fef384",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "62fef384",
        "outputId": "fdd63788-2f3e-43bd-d178-d92df6d68d46"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "grader.check(\"priors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "49fc8a6a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "49fc8a6a",
        "outputId": "eca35e10-9ffa-4ee7-ba4f-b83711a68795"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madison  prior: 0.2273\n",
            "Hamilton prior: 0.7727\n"
          ]
        }
      ],
      "source": [
        "print(f\"Madison  prior: {prior_madison:.4f}\\n\"\n",
        "      f\"Hamilton prior: {prior_hamilton:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a485b4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a8a485b4"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What do these probabilities tell us about how we might predict the class of a _Federalist_ document _prior_ to looking at the actual content of the document? (That's why these probabilities are called \"priors\".)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_priors\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c06d9cc1",
      "metadata": {
        "id": "c06d9cc1"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Given an unknown _Federalist_ document, and without knowing anything more about the actual content of the document or about these two classes ‚Äì the best we can do is to predict the majority class of the training set.\n",
        "\n",
        "If we assume that the examples of the training set were chosen randomly and the example was chosen by the same probability as the distribution of the classes ‚Äì the group relative part of the training set should be (in the limit) the distribution of the group. We would like to choose the group with the highest probability, so we would like to chose the group whose training set part was maximal.\n",
        "\n",
        "Therefore, we might predict the class of such a document as Hamilton, since it is the majority group label."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9921350",
      "metadata": {
        "id": "b9921350"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "## Estimating the likelihood probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef87208b",
      "metadata": {
        "id": "ef87208b"
      },
      "source": [
        "Now for the likelihood probabilities, the conditional probability of a word given a class. For each likelihood $\\Prob(x_j \\given y_i)$, we need to estimate a value. We'll do so by simply counting the number of training examples with feature value $x_j$ that are labeled $y_i$ (notated as $\\cnt{x_j, y_i}$) as a proportion of the overall number of words labeled as $y_i$, that is,\n",
        "\n",
        "$$ \\Prob(x_j \\given y_i) \\approx \\frac{\\cnt{x_j, y_i}}{\\sum_k \\cnt{x_k, y_i}} $$\n",
        "\n",
        "Again, for the text case, each token counts as an instance of the corresponding word type in a training example. Note that $\\sum_k \\cnt{x_k, y_i}$ is not the same as $\\cnt{y_i}$.\n",
        " \n",
        "We've provided a small table that shows, for each label (author) and each of the four word types of interest, how many tokens of the type occurred in training examples with that label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9eb7c682",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9eb7c682",
        "outputId": "b6614c36-28b7-4a59-d3dc-f362bfaa4a63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                on    upon   there  whilst\n",
            "Hamilton       390     377     369       1\n",
            "Madison        308       7      32      12\n"
          ]
        }
      ],
      "source": [
        "def counts(dataset, label, index):\n",
        "    \"\"\"Returns the total count for `index` for examples with the \n",
        "       given `label`\"\"\"\n",
        "    return sum([example['counts'][index] \n",
        "                for example in dataset \n",
        "                if example['authors'] == label])\n",
        "\n",
        "# print a table header\n",
        "print(f\"{'':10}\", end=\"\")\n",
        "for i in range(4):\n",
        "    print(f\"{keywords[i]:>8}\", end=\"\")\n",
        "print()\n",
        "# print table entries for each label\n",
        "for label in classes:\n",
        "    print(f\"{label:10}\", end=\"\")\n",
        "    for i in range(4):\n",
        "        print(f\"{counts(training, label, i):8}\", end=\"\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8bb316a",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "f8bb316a"
      },
      "source": [
        "Given the counts in this table, what would an estimate be for the probability that a given word would be \"whilst\" given that the document was authored by Madison, that is, $\\Prob(\\mathrm{whilst} \\given \\mathrm{Madison})$?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: prob_whilst_madison\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "34298195",
      "metadata": {
        "id": "34298195"
      },
      "outputs": [],
      "source": [
        "#TODO - Define this variable to be the specified probability.\n",
        "prob_whilst_madison = counts(training, \"Madison\", 3) / sum([counts(training, \"Madison\", i) for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "7b43049e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "7b43049e",
        "outputId": "2a0500af-005b-4f79-fa10-8f6ad8df2a57"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "grader.check(\"prob_whilst_madison\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0699b20",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e0699b20"
      },
      "source": [
        "What about the probability $\\Prob(\\mathrm{on} \\given \\mathrm{Hamilton})$?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: prob_on_hamilton\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "038ef343",
      "metadata": {
        "id": "038ef343"
      },
      "outputs": [],
      "source": [
        "#TODO - Define this variable to be the specified probability.\n",
        "prob_on_hamilton = counts(training, \"Hamilton\", 0) / sum([counts(training, \"Hamilton\", i) for i in range(4)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "602c8879",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "602c8879",
        "outputId": "089686cf-0b48-4de1-ec1d-0eb8b7836f0c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "grader.check(\"prob_on_hamilton\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04f9f121",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "04f9f121"
      },
      "source": [
        "Consider a sample text \n",
        "\n",
        "> **whilst** depending neither **on** the American government nor **on** the British\n",
        "\n",
        "What would the Naive Bayes method estimate be for the likelihood of this sentence if it was by Hamilton? By Madison? (You should of course ignore all the tokens in our little sample text except for tokens of the four keyword types. (We've boldfaced their occurrences.) With a full-blown NB analysis, we'd be using *all* of the words in the text.)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: likelihoods\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "b61e67ca",
      "metadata": {
        "id": "b61e67ca"
      },
      "outputs": [],
      "source": [
        "#TODO - Define the variables to be the corresponding likelihood probabilities.\n",
        "prob_on_madison = counts(training, \"Madison\", 0) / sum([counts(training, \"Madison\", i) for i in range(4)])\n",
        "prob_whilst_hamilton = counts(training, \"Hamilton\", 3) / sum([counts(training, \"Hamilton\", i) for i in range(4)])\n",
        "\n",
        "likelihood_hamilton =  prob_whilst_hamilton * prob_on_hamilton * prob_on_hamilton\n",
        "likelihood_madison =  prob_whilst_madison * prob_on_madison * prob_on_madison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "a1de5c74",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "a1de5c74",
        "outputId": "e3cdeeb3-cf30-4763-9946-ee49fa7a2f19"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "grader.check(\"likelihoods\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "2ac7534d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2ac7534d",
        "outputId": "313161aa-8add-45cd-dec0-631576ddb593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madison  likelihood: 0.024604\n",
            "Hamilton likelihood: 0.000103\n"
          ]
        }
      ],
      "source": [
        "print(f\"Madison  likelihood: {likelihood_madison:4f}\\n\"\n",
        "      f\"Hamilton likelihood: {likelihood_hamilton:4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f8f9e5fa",
      "metadata": {
        "id": "f8f9e5fa"
      },
      "source": [
        "## Posterior probabilities"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0bfdb84",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "a0bfdb84"
      },
      "source": [
        "We're almost there. We simply need to combine the prior probabilities and the likelihood probabilities for each class to form the posterior, and select the largest one. As a reminder, we don't actually calculate the posterior _probability_ because we aren't dividing through by $\\Prob(\\vect{x})$. Instead, we get something like a posterior _score_.\n",
        "\n",
        "Calculate the posteriors for the two classes, and then specify which class ‚Äì Hamilton or Madison ‚Äì the NB method would predict for the sample text.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: posteriors\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "362846bc",
      "metadata": {
        "id": "362846bc"
      },
      "outputs": [],
      "source": [
        "#TODO - Define the variables to be the corresponding posterior probabilities, \n",
        "#       and the classification of the sample phrase.\n",
        "posterior_madison = prior_madison * likelihood_madison\n",
        "posterior_hamilton = prior_hamilton * likelihood_hamilton\n",
        "sample_classification = \"Madison\" if posterior_madison > posterior_hamilton else \"Hamilton\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "c7c93d88",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "c7c93d88",
        "outputId": "15f6ac59-7b44-4da3-d998-936a5e9d27b2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "grader.check(\"posteriors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e71b89e0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e71b89e0",
        "outputId": "1dd12dcf-4363-4e0d-e95b-9aba0b16d761"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madison  posterior: 0.005592\n",
            "Hamilton posterior: 0.000080\n",
            "Sample classification: Madison\n"
          ]
        }
      ],
      "source": [
        "print(f\"Madison  posterior: {posterior_madison:4f}\\n\"\n",
        "      f\"Hamilton posterior: {posterior_hamilton:4f}\\n\"\n",
        "      f\"Sample classification: {sample_classification}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46caab1d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "46caab1d"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Is the NB-predicted classification the same as or different from the classification based on the priors? Why?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_nb_v_priors\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e64c5f0f",
      "metadata": {
        "id": "e64c5f0f"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "No, the Na√Øve-Bayes predicted classification does not match the classification based on the priors.\n",
        "\n",
        "The Na√Øve-Bayes predicted class is Madison since Madison posterior is higher than Hamilton posterior ($0.005592 > 0.000080$). However, Madison prior is smaller than Hamilton prior ($0.2273 < 0.7727$).\n",
        "\n",
        "The reason for the difference is the likelihood ‚Äì Madison likelihood is much higher than Hamilton likelihood ($0.024604 > 0.000103$), so after multiplying these values with the priors, the result for Madison is higher.\n",
        "\n",
        "We should note that the prior values do not refer to the content, so it is not surprising that by analyzing the content the prediction has changed."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "be06ebad",
      "metadata": {
        "id": "be06ebad"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "## A practical issue\n",
        "\n",
        "The computations of what we've been calling the posterior scores\n",
        "$$\\Prob(y_i \\given \\vect{x}) \\approx \\Prob(y_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given y_i)$$\n",
        "involve the multiplication of many extremely small numbers. This is a recipe for [_arithmetic underflow_](https://en.wikipedia.org/wiki/Arithmetic_underflow), leading to garbage outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "174574d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "174574d6",
        "outputId": "cbfb58a4-363d-4d5e-eec2-6cff0752d6eb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# An example for an arithmetic unferflow\n",
        "2**(-1076)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f465fe",
      "metadata": {
        "id": "97f465fe"
      },
      "source": [
        "Instead, rather than maximizing the posterior, we can maximize its logarithm. Since the logarithm function is monotonic (see the next cell for a figure), whichever  ùëñ  maximizes the posterior maximizes its log as well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "2994c2b3",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "2994c2b3",
        "outputId": "06d10243-7e72-4545-c06d-69e8f9396961"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def log_plot():\n",
        "    x = torch.linspace(1e-10, 1, 100)\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.plot(x, torch.log2(x), label = \"log_2\")\n",
        "    plt.title(\"Monotonicity of logarithm\")\n",
        "    plt.legend()\n",
        "    \n",
        "log_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d5a5b5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "74d5a5b5"
      },
      "source": [
        "The log of the posterior is $\\log \\left(\\Prob(y_i) \\cdot \\prod_{j=1}^m \\Prob(x_j \\given y_i)\\right) = \\log\\Prob(y_i) + \\sum_{j=1}^m \\log\\Prob(x_j \\given y_i)$\n",
        "so that the calculation now involves the sum of a bunch of numbers rather than the product. In practice, this computation is much more robust.\n",
        "\n",
        "> A log-of-probability value is referred to, colloquially if not quite accurately, as a *logit*, because of a resemblance to the values of [the logit function](https://en.wikipedia.org/wiki/Logit).\n",
        "\n",
        "Calculate the log of the posterior for Madison for the sample text by summing up all of the pertinent logits, and similarly for Hamilton. Use the base 2 logarithm.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: log_posteriors\n",
        "-->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "cb4d9152",
      "metadata": {
        "id": "cb4d9152"
      },
      "outputs": [],
      "source": [
        "#TODO - Calculate the log of the posterior for Madison by summing up all \n",
        "#       of the pertinent parts.\n",
        "from math import log2\n",
        "log_posterior_madison = math.log2(prob_whilst_madison) + math.log2(prob_on_madison) + math.log2(prob_on_madison)\n",
        "log_posterior_hamilton = math.log2(prob_whilst_hamilton) + math.log2(prob_on_hamilton) + math.log2(prob_on_hamilton)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "0c76b6f2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "0c76b6f2",
        "outputId": "f142878e-10a2-43bf-9960-4acdd1dfde8c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "grader.check(\"log_posteriors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "2eda749d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "2eda749d",
        "outputId": "b2196d6e-7716-49e8-b015-7068f9b542f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Madison  log posterior:   -5.345\n",
            "Hamilton log posterior:  -13.238\n"
          ]
        }
      ],
      "source": [
        "print(f\"Madison  log posterior: {log_posterior_madison:8.3f}\\n\"\n",
        "      f\"Hamilton log posterior: {log_posterior_hamilton:8.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fffbfbfc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "fffbfbfc"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Which one of the two is larger? Does this accord with your expectation?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_posterior\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "acaa8709",
      "metadata": {
        "id": "acaa8709"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Madison log posterior is higher than Hamilton log posterior, since $-5.345 > - 13.238$ . Therefore, the predicted class is Madison.\n",
        "\n",
        "As we saw earlier, we predicted the class as Madison, so it matches our expectations."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b4886ac",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "1b4886ac"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "# Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98abbfd0",
      "metadata": {
        "id": "98abbfd0"
      },
      "source": [
        "**Answer:**\n",
        "* The length was great, maching exactly the allocated time.\n",
        "* Yes, the reading were relevant and appropiate for the lab.\n",
        "* The points of the exercise were evaetually clear, yet some of the instructions were not clear.\n",
        "* We think the lab was good."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b5d02b5",
      "metadata": {
        "id": "4b5d02b5"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of lab 3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7bbbb5d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "id": "e7bbbb5d"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "b87f11c1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "id": "b87f11c1",
        "outputId": "271e1f59-2f53-4a87-c97f-bb7ad17cca13"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "likelihoods:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "log_posteriors:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "posteriors:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "priors:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "prob_on_hamilton:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "prob_whilst_madison:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>likelihoods:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>log_posteriors:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>posteriors:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>priors:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>prob_on_hamilton:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>prob_whilst_madison:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ],
      "source": [
        "grader.check_all()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 1-3: Naive Bayes classification",
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}