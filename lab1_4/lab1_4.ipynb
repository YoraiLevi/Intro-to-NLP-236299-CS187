{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "a78ff25c",
        "outputId": "774391ba-bdf8-4d52-b31e-9c7b6fce7c5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Please do not change this cell because some hidden tests might depend on it.\n",
        "import os\n",
        "\n",
        "# Otter grader does not handle ! commands well, so we define and use our\n",
        "# own function to execute shell commands.\n",
        "def shell(commands, warn=True):\n",
        "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n",
        "     \n",
        "       Prints the result to stdout and returns the exit status. \n",
        "       Provides a printed warning on non-zero exit status unless `warn` \n",
        "       flag is unset.\n",
        "    \"\"\"\n",
        "    file = os.popen(commands)\n",
        "    print (file.read().rstrip('\\n'))\n",
        "    exit_status = file.close()\n",
        "    if warn and exit_status != None:\n",
        "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n",
        "    return exit_status\n",
        "\n",
        "shell(\"\"\"\n",
        "ls requirements.txt >/dev/null 2>&1\n",
        "if [ ! $? = 0 ]; then\n",
        " rm -rf .tmp\n",
        " git clone https://github.com/cs236299-2023-spring/lab1-4.git .tmp\n",
        " mv .tmp/tests ./\n",
        " mv .tmp/requirements.txt ./\n",
        " rm -rf .tmp\n",
        "fi\n",
        "pip install -q -r requirements.txt\n",
        "\"\"\")"
      ],
      "id": "a78ff25c"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7fe3e0a4"
      },
      "outputs": [],
      "source": [
        "# Initialize Otter\n",
        "import otter\n",
        "grader = otter.Notebook()"
      ],
      "id": "7fe3e0a4"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "id": "d9145930"
      },
      "source": [
        "%%latex\n",
        "\\newcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\newcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\newcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\newcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\newcommand{\\Prob}{\\Pr}\n",
        "\\newcommand{\\given}{\\,|\\,}"
      ],
      "id": "d9145930"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49cbea0a"
      },
      "source": [
        "$$\n",
        "\\renewcommand{\\vect}[1]{\\mathbf{#1}}\n",
        "\\renewcommand{\\cnt}[1]{\\sharp(#1)}\n",
        "\\renewcommand{\\argmax}[1]{\\underset{#1}{\\operatorname{argmax}}}\n",
        "\\renewcommand{\\softmax}{\\operatorname{softmax}}\n",
        "\\renewcommand{\\Prob}{\\Pr}\n",
        "\\renewcommand{\\given}{\\,|\\,}\n",
        "$$"
      ],
      "id": "49cbea0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bcd66f96"
      },
      "source": [
        "# Course 236299\n",
        "## Lab 1-4 – Discriminative methods for classification"
      ],
      "id": "bcd66f96"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05185086"
      },
      "source": [
        "In this lab, you'll apply a discriminative classification method to the _Federalist_ papers' authorship attribution problem, the logistic regression (perceptron) model. \n",
        "\n",
        "After this lab, you should be able to\n",
        "\n",
        "* Derive the basic equations for the logistic regression classification method;\n",
        "* Perform the forward computation to generate the class that a logistic regression model predicts for some input;\n",
        "* Calculate a loss for that prediction, the cross-entropy loss;\n",
        "* Update the parameters of a logistic regression model by stochastic gradient descent."
      ],
      "id": "05185086"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ff9cdde"
      },
      "source": [
        "New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful:\n",
        "\n",
        "* `numpy.exp`\n",
        "* `torch.dot`"
      ],
      "id": "5ff9cdde"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60b150a4"
      },
      "source": [
        "# Preparation – Loading packages and data"
      ],
      "id": "60b150a4"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "be112559"
      },
      "outputs": [],
      "source": [
        "# Please do not change these imports because some hidden tests might depend on them.\n",
        "# You can add a cell below if you need to import anything else.\n",
        "import json\n",
        "import math\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "matplotlib.style.use('tableau-colorblind10')\n",
        "import os\n",
        "import torch\n",
        "import wget\n",
        "\n",
        "from torch import log2\n",
        "from pprint import pprint"
      ],
      "id": "be112559"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "c16ec863"
      },
      "outputs": [],
      "source": [
        "# Download and read the Federalist data from the json file\n",
        "os.makedirs('data', exist_ok=True)\n",
        "wget.download('https://github.com/nlp-236299/data/raw/master/Federalist/federalist_data.json', out='data/')\n",
        "with open('data/federalist_data.json', 'r') as fin:\n",
        "    dataset = json.load(fin)"
      ],
      "id": "c16ec863"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "f4cc2b2d"
      },
      "outputs": [],
      "source": [
        "# As before, we tensorize the count vectors...\n",
        "for example in dataset:\n",
        "    example['counts'] = torch.tensor(example['counts']).type(torch.float)\n",
        "    \n",
        "# ...and extract the papers by either of Madison and Hamilton \n",
        "# to serve as training data....\n",
        "training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n",
        "                       dataset))"
      ],
      "id": "f4cc2b2d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2b8749d9"
      },
      "source": [
        "# Logistic regression\n",
        "\n",
        "You've read about logistic regression, a method for classification that is discriminative rather than generative. Like the generative Naive Bayes method, each example is characterized by a vector of features (the counts of word types for a text, say, as for the _Federalist_ example that we've been using). In logistic regression, each such feature is assigned a weight, and the score for an example is given by weighting each feature value by its weight and summing the result; that is, the score is the dot product of the feature vector and the weight vector. Let's take an example, one of the Federalist papers. We'll extract from the training data the counts for the first example in the training set. That's the feature vector for this example."
      ],
      "id": "2b8749d9"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3661dd46",
        "outputId": "ae0e1a32-76a7-4bb9-ee41-86b37b76ea5a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9., 6., 2., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "training0_counts = training[0]['counts']\n",
        "training0_counts"
      ],
      "id": "3661dd46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "164877dd"
      },
      "source": [
        "Suppose the weights for the features are as given by the following vector:"
      ],
      "id": "164877dd"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "f7e51af8"
      },
      "outputs": [],
      "source": [
        "weights = torch.tensor([-.1, .2, .3, -1])"
      ],
      "id": "f7e51af8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60d6c648"
      },
      "source": [
        "What would the weighted sum of the counts with these weights be? Feel free to use `torch.dot` to take the dot product for you. (Alternatively, you can also use basic tensor operations such as `*` and `.sum()`.)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: wtd_sum\n",
        "-->"
      ],
      "id": "60d6c648"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "e5dd6caa"
      },
      "outputs": [],
      "source": [
        "#TODO: Take the weighted sum of `training0_counts` with `weights`\n",
        "wtd_sum = torch.dot(training0_counts,weights)"
      ],
      "id": "e5dd6caa"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "4c9185bb",
        "outputId": "1f665da8-1a5f-49af-9c56-12e13869c6d9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "grader.check(\"wtd_sum\")"
      ],
      "id": "4c9185bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "806eb767"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What is the range of possible values that such a weighted sum can take on?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_possible_sums\n",
        "manual: true\n",
        "-->"
      ],
      "id": "806eb767"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b552967"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The range of the possible values that such a weighted sum can take on is $(-\\infty,+\\infty)$ .\n",
        "\n",
        "Since the allowed weights are $(-\\infty,+\\infty)$ for each index, the multiplication and the summation can result in any value of $(-\\infty,+\\infty)$."
      ],
      "id": "6b552967"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30f7bbce"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "In order to have a standard way of comparing these numbers, it helps to be able to place them on a fixed scale, from 0 to 1, say. This way, they can be interpreted as probabilities. We use the _logistic function_ ($\\sigma$) to carry out this conversion:\n",
        "\n",
        "$$ \\sigma(x) = \\frac{1}{1 + e^{-k x}}$$\n",
        "\n",
        "In addition to its argument $x$, the function takes an additional parameter $k$, which we will explore shortly.\n",
        "\n",
        "Define a function `sigma` implementing the logistic function. (We've established a default value for `k` of 1 in the header line below.)\n",
        "\n",
        "> Hint: For exponentiation, you can use [the `exp` function provided by `torch`](https://pytorch.org/docs/stable/generated/torch.exp.html).\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: sigma\n",
        "-->"
      ],
      "id": "30f7bbce"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "260c885f"
      },
      "outputs": [],
      "source": [
        "#TODO: Implement the logistic function.\n",
        "def sigma(x, k=1):\n",
        "    \"\"\"Computes sigma(x) = 1 / (1 + exp(-kx)).\n",
        "    Arguments:\n",
        "      x: a tensor\n",
        "    Returns: sigma applied to each element of x (a tensor)\"\"\"\n",
        "    return 1 / (1 + torch.exp(-k*x))"
      ],
      "id": "260c885f"
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "dc987765",
        "outputId": "fb5eeb58-e471-46c4-a456-9a929998017d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "grader.check(\"sigma\")"
      ],
      "id": "dc987765"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4330c48d"
      },
      "source": [
        "To get a sense of the logistic function, we graph it for several values of $k$."
      ],
      "id": "4330c48d"
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 452
        },
        "id": "e905bc46",
        "outputId": "769f7540-3c5e-474b-a3e5-f3ce65e9063c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def sigma_plot():\n",
        "    x = torch.linspace(-2, 2, 100)\n",
        "    fig, ax = plt.subplots()\n",
        "    for k in [0, 1, 2, 10]:\n",
        "        ax.plot(x, sigma(x, k), label = f\"k = {k}\")\n",
        "    plt.title(\"Logistic functions\")\n",
        "    plt.legend()\n",
        "    \n",
        "sigma_plot()"
      ],
      "id": "e905bc46"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "657e0a42"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What does the $k$ parameter do to the logistic function?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "manual: true\n",
        "name: open_response_k\n",
        "-->"
      ],
      "id": "657e0a42"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3fa2ecd"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The $k$ parameter defines the steepness of the S-shape graph. Larger $k$ values result in a bigger slope near $0$, and smaller $k$ values result in a smaller slope near $0$.\n",
        "\n",
        "Semantically, the $k$ parameter defines how much \"forgiving\" the function should be for numbers close to $0$, as the function measures the sign of the given input. Higher $k$ values result in lower \"forgiveness\" and higher sensitivity for values near $0$."
      ],
      "id": "b3fa2ecd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20a9d41c"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** In the figure, the logistic function looks to be radially symmetric. In particular, it appears that $\\sigma(x) = 1 - \\sigma(-x)$. (If so, we can use a simple thresholding for classificiation purposes, $\\sigma(x) > 0.5$ to capture $x > 0$.)\n",
        "\n",
        "Prove the identity $\\sigma(x) = 1 - \\sigma(-x)$.\n",
        "\n",
        "> Hint: *Don't* get hung up on this problem during the lab session. You can always come back to it later.\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "manual: true\n",
        "name: open_response_sigma_identity\n",
        "-->"
      ],
      "id": "20a9d41c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "558198bb"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Defining the function –\n",
        "$$\\sigma(x) = \\frac{1}{1 + e^{-k x}}$$\n",
        "\n",
        "After performing mathematical simplifications –\n",
        "$$1-\\sigma(-x) = 1-\\frac{1}{1 + e^{-k (-x)}} = 1-\\frac{1}{1 + e^{kx}} = \\frac{1 + e^{kx}-1}{1 + e^{kx}}=\\frac{e^{kx}}{1 + e^{kx}}=\\frac{1}{e^{-kx}\\cdot(1 + e^{kx})}=\\frac{1}{1 + e^{-kx}}=\\sigma(x)$$\n",
        "\n",
        "So the required is indeed an identity –\n",
        "$$\\sigma(x) = 1-\\sigma(-x)$$\n",
        "\n",
        "$\\blacksquare$"
      ],
      "id": "558198bb"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31ad9bc7"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "The logistic function, when applied to the weighted average above is greater than 0.5."
      ],
      "id": "31ad9bc7"
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6590e1b1",
        "outputId": "26c13b38-b154-4984-c877-18cd42413915"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(0.7109)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "sigma(wtd_sum)"
      ],
      "id": "6590e1b1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5795966f"
      },
      "source": [
        "In the _Federalist Papers_ classification problem, there are only two classes, so we can use this single value to determine the classification. For an input feature vector $\\vect{x}$ and weight vector $\\vect{w}$, we'll take the model to predict the probability of the author being Hamilton (say), as\n",
        "\n",
        "$$\\Prob(\\mathrm{Hamilton} \\given \\mathbf{x}) = \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$$\n",
        "\n",
        "Therefore, \n",
        "\n",
        "$$\\Prob(\\mathrm{Madison} \\given \\mathbf{x}) = 1 - \\sigma(\\mathbf{w} \\cdot \\mathbf{x})$$\n",
        "\n",
        "since there are only two classes.\n",
        "\n",
        "> When there are more than two classes, we'd use a generalization of the sigmoid function, called [softmax](https://en.wikipedia.org/wiki/Softmax_function).\n",
        "\n",
        "Define a function `predict_lr` that calculates the probability of Hamilton being the author of an example text, given a weight vector and the feature vector for the example text.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: predict_lr\n",
        "-->"
      ],
      "id": "5795966f"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2650811a"
      },
      "outputs": [],
      "source": [
        "#TODO: Calculate the probability of Hamilton being the author\n",
        "def predict_lr(weights, features):\n",
        "    return sigma(torch.dot(weights,features))"
      ],
      "id": "2650811a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a59eab20"
      },
      "source": [
        "This is the *forward computation* for the logistic regression model, calculating its output prediction from some inputs. Next we turn to the *backward computation*, calculating the updates to the parameters based on any error in the predicted output, as measured by a loss function."
      ],
      "id": "a59eab20"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95265faa"
      },
      "source": [
        "# Using a logistic regression model to predict Federalist authorship"
      ],
      "id": "95265faa"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99927860"
      },
      "source": [
        "Consider the following two training examples (examples 0 and 9) from the _Federalist_ training dataset:"
      ],
      "id": "99927860"
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5702ea75",
        "outputId": "44296de3-66d8-412c-f889-3c01ea3a45cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'authors': 'Hamilton',\n",
            " 'counts': tensor([9., 6., 2., 0.]),\n",
            " 'number': '1',\n",
            " 'title': 'General Introduction'}\n",
            "{'authors': 'Madison',\n",
            " 'counts': tensor([17.,  0.,  0.,  1.]),\n",
            " 'number': '14',\n",
            " 'title': 'Objections to the Proposed Constitution from Extent of Territory '\n",
            "          'Answered'}\n"
          ]
        }
      ],
      "source": [
        "for example in [0, 9]:\n",
        "    pprint(training[example])"
      ],
      "id": "5702ea75"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8d98de21"
      },
      "source": [
        "As above, a logistic regression model is defined by a vector of weights $\\mathbf{w}$, like these:"
      ],
      "id": "8d98de21"
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "34da3219",
        "outputId": "a9d879de-e074-4f54-bf53-528797e89dc4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1000,  0.2000,  0.3000, -1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "weights"
      ],
      "id": "34da3219"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10296cc2"
      },
      "source": [
        "Calculate the predicted Hamilton probabilities for the two examples (examples 0 and 9) above.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: prob_hamilton_examples\n",
        "-->"
      ],
      "id": "10296cc2"
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "adc8914f"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "prob_hamilton_example0 = predict_lr(weights,training[0]['counts'])\n",
        "prob_hamilton_example9 = predict_lr(weights,training[9]['counts'])"
      ],
      "id": "adc8914f"
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "0f3c7704",
        "outputId": "218be6be-e86f-4a6b-ccc6-510bdb74ea16"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "grader.check(\"prob_hamilton_examples\")"
      ],
      "id": "0f3c7704"
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9e21e6b4",
        "outputId": "eef675e1-2e32-4ed0-d089-c2fe3065e8af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example 0 prob of Hamilton: 0.711\n",
            "example 9 prob of Hamilton: 0.063\n"
          ]
        }
      ],
      "source": [
        "print(f\"example 0 prob of Hamilton: {prob_hamilton_example0:.3f}\\n\"\n",
        "      f\"example 9 prob of Hamilton: {prob_hamilton_example9:.3f}\")"
      ],
      "id": "9e21e6b4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4578c6a9"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What does this model predict about the two training examples? Is it correct?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_original_model_prediction\n",
        "manual: true\n",
        "-->"
      ],
      "id": "4578c6a9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46ee058c"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "For example #0, the model predicts _Hamilton_ since the probability value is higher than 0.5. This prediction is indeed correct since the expected label is indeed Hamilton (as can be seen earlier).\n",
        "\n",
        "For example #9, the model predicts _Madison_ (= _Not Hamilton_) since the probability value is smaller than 0.5 and the prediction is binary (so _Madison_ and _Not Hamilton_ are the same). This prediction is indeed correct since the expected label is indeed Madison (as can be seen earlier)."
      ],
      "id": "46ee058c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "94a73468"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# Training a logistic regression model\n",
        "\n",
        "Of course, this is just one of a gazillion models (weight vectors), since we could have set the weights in all kinds of ways. How should we come up with a _good_ model, i.e., a good setting of the weights? Ideally, we'd try to find a set of weights that predicts all of the training data well. This is the problem of _training_ a logistic regression model. Let's try another set of weights:"
      ],
      "id": "94a73468"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "c2abe9f8"
      },
      "outputs": [],
      "source": [
        "weights_new = torch.tensor([0.1, 0.2, 0.3, -5.])"
      ],
      "id": "c2abe9f8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55b78a2a"
      },
      "source": [
        "Calculate the probabilities generated by the model for these weights.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: prob_hamilton_examples_new\n",
        "-->"
      ],
      "id": "55b78a2a"
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "3d54757f"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "prob_hamilton_example0_new = predict_lr(weights_new,training[0]['counts'])\n",
        "prob_hamilton_example9_new = predict_lr(weights_new,training[9]['counts'])"
      ],
      "id": "3d54757f"
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "be0adbaf",
        "outputId": "2d7a35f8-a807-4d86-c3e5-c99de66ba370"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "grader.check(\"prob_hamilton_examples_new\")"
      ],
      "id": "be0adbaf"
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "831b78c6",
        "outputId": "7915750c-20ec-4217-81e5-a05ac5afcb34"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "example 0 prob: 0.937\n",
            "example 9 prob: 0.036\n"
          ]
        }
      ],
      "source": [
        "print(f\"example 0 prob: {prob_hamilton_example0_new:.3f}\\n\" \n",
        "      f\"example 9 prob: {prob_hamilton_example9_new:.3f}\")"
      ],
      "id": "831b78c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2c145c6"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:**  Is this a better model, a worse model, or neither, at least as far as the two sample examples are concerned? \n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_new_model_prediction\n",
        "manual: true\n",
        "-->"
      ],
      "id": "f2c145c6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f058e1f0"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "For these two examples, the model predicts the same as the previous one.\n",
        "\n",
        "If the two models predict exactly the same, _id est_ for **every** possible example the probabilty given by the first model is higher than $0.5$ if-and-only-if the probabilty given by the second model is higher than $0.5$ (taking into account numerical precision issues) – the second model is neither worse nor better than the first model, as it is always agree with.\n",
        "\n",
        "However, these assumptions are usually not practical. The \"self-confidence\" of the second model compared to the first one, resulting in higher probabilty value for the #0 example and smaller probabilty value for the #9 example, may indicate that the second model can seperete the two groups better and thus make less mistakes. Therefore, it is possible that the second model is better than the first one.\n",
        "\n",
        "_[This answer is based on a discussion held with assistant professor Yonatan in class]._"
      ],
      "id": "f058e1f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dbc5874"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "An ideal model would give a probability of 1 to the Hamilton examples and a 0 to the Madison examples.\n",
        "For the two sample examples, you'll have noticed that the new weights generate not 1 and 0, respectively, but numbers quite a bit closer to 1 and 0.\n",
        "\n",
        "We could continue trying different weight values to try to improve the performance of the model on these training examples (and others) by trial and error, but a more systematic method is needed. We define a *loss function*, which specifies how bad a model is, and try to minimize the loss function by *stochastic gradient descent*.\n",
        "\n",
        "We'll do a few steps of the process here by hand. It's sufficiently tedious that it's far better to deploy computers on the task, which we'll do in the next lab.\n",
        "\n",
        "# Cross-entropy loss function\n",
        "\n",
        "We'll use the cross-entropy loss function. For an example $i$, we'll use $\\mathbf{x}^{(i)}$ for the feature vector, and $y^{(i)}$ for the actual (gold) label (1 or 0, 1 for Hamilton and 0 for Madison). The predicted probability of the label for the $i$-th example being 1 will be as per the logistic regression model $\\sigma(\\mathbf{w} \\cdot \\mathbf{x}^{(i)})$, which we will call $\\hat{y}^{(i)}$.\n",
        "\n",
        "The cross-entropy loss for example $i$ as per a model $\\mathbf{w}$ is\n",
        "$$L_{CE}(\\mathbf{w}) = -(y^{(i)} \\log \\hat{y}^{(i)} + (1-y^{(i)}) \\log (1-\\hat{y}^{(i)}))$$\n",
        "\n"
      ],
      "id": "4dbc5874"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7fac7a71"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** What is the minimum possible value of the cross-entropy loss above? When is that achieved? (Note that while $0\\log 0$ is undefined, $\\lim_{x\\to0^{+}}x\\log x=0$.)\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_ce_loss\n",
        "manual: true\n",
        "-->"
      ],
      "id": "7fac7a71"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2a301c1"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "First, we define the continuos function $f(x) = -\\left(x\\log x+\\left(1-x\\right)\\log\\left(1-x\\right)\\right)$. This function has a minimum for $x = 1, f(x) = 0$ (and an additional infimum for $x=0, lim_{x\\to0^+}f(x) = 0$).\n",
        "\n",
        "Due to the characters of analysis of functions, the cross-entropy function (with the discrete values) shares this minimum value, so its minimum possible value is $0$. This is achieved when the model predicted the current example correctly, with the maximal distance of the probability value from $0.5$."
      ],
      "id": "c2a301c1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3f6fabef"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "We define a function to compute the cross-entropy loss for a particular model (weight vector), example (feature vector), and gold label:"
      ],
      "id": "3f6fabef"
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "3729985e"
      },
      "outputs": [],
      "source": [
        "def loss(weights, features, correct):\n",
        "    \"\"\"Returns the cross-entropy loss for a weight vector, a feature\n",
        "       vector and a gold label `correct`.\"\"\"\n",
        "    y_hat = predict_lr(weights, features)\n",
        "    return -(correct * log2(y_hat)\n",
        "             + (1 - correct) * log2(1 - y_hat))"
      ],
      "id": "3729985e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfe5c34d"
      },
      "source": [
        "Use the `loss` function to determine the cross-entropy loss for example 0 for the original model that we used (`weights`), and for the new model (`weights_new`).\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: loss_example0_old_and_new\n",
        "-->"
      ],
      "id": "cfe5c34d"
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "3599d8cc"
      },
      "outputs": [],
      "source": [
        "#TODO: Calculate loss for training[0] under `weights` and `weights_new`\n",
        "loss_example0_old = loss(weights,training[0]['counts'],int(training[0]['authors']==\"Hamilton\"))\n",
        "loss_example0_new = loss(weights_new,training[0]['counts'],int(training[0]['authors']==\"Hamilton\"))"
      ],
      "id": "3599d8cc"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "b246190d",
        "outputId": "8b59f8f1-10d2-4c6c-81ba-64f8e87857b5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "grader.check(\"loss_example0_old_and_new\")"
      ],
      "id": "b246190d"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ddf0e830",
        "outputId": "92035240-e693-40a8-f4d2-de222122e8f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old model loss: 0.492\n",
            "New model loss: 0.094\n"
          ]
        }
      ],
      "source": [
        "print(f\"Old model loss: {loss_example0_old:.3f}\\n\"\n",
        "      f\"New model loss: {loss_example0_new:.3f}\")"
      ],
      "id": "ddf0e830"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5de736f0"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Which of the models is better (at least on this example)?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_loss_on_example\n",
        "manual: true\n",
        "-->"
      ],
      "id": "5de736f0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a34b087d"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Considering only this example, the new model is better. We expect the model to minimize the loss, so it is 'more confident' with the result.\n",
        "\n",
        "The old model loss is higher than the new model loss ($0.492 > 0.094$), so the new one is better (lower loss is better)."
      ],
      "id": "a34b087d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73eac9d8"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# Optimizing the weights with the gradient of the loss function\n",
        "\n",
        "We want to find the weights that minimize the loss function. We use gradient descent:\n",
        "\n",
        "1. Find the gradient of the loss function, the direction in which it is increasing fastest.\n",
        "2. Take a step in the opposite direction.\n",
        "3. Repeat.\n",
        "\n",
        "For cross-entropy loss, recall that the partial derivative of the loss function with respect to a single weight $w_j$ is\n",
        "\n",
        "$$ \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_j} = (\\hat{y} - y) \\cdot x_j $$\n",
        "\n",
        "> At the end of this subsection, we give a problem that explores how this gradient is derived, but you can just assume it for the time being.\n",
        "\n",
        "The gradient combines these partial derivatives for all of the weights.\n",
        "\n",
        "$$ \\nabla L_{CE}(\\mathbf{w}) = \\left[ \\begin{array}{c}\n",
        "    \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_1}\\\\\n",
        "    \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_2}\\\\\n",
        "    \\vdots \\\\\n",
        "    \\frac{\\partial L_{CE}(\\mathbf{w})}{\\partial w_m}\n",
        "    \\end{array} \\right] $$\n",
        "\n",
        "Let's work out an example, using example 0. The counts for example 0 are"
      ],
      "id": "73eac9d8"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7db63e87",
        "outputId": "53a7bbe4-5cbb-458a-ee6f-d7c467ba779f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([9., 6., 2., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "training[0]['counts']"
      ],
      "id": "7db63e87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a8582be"
      },
      "source": [
        "and the original weights, recall, were"
      ],
      "id": "1a8582be"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f9bbbc3b",
        "outputId": "4ccda9b6-13e0-4dcd-94d6-77430f9cea98"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.1000,  0.2000,  0.3000, -1.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ],
      "source": [
        "weights"
      ],
      "id": "f9bbbc3b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eae24505"
      },
      "source": [
        "What is the gradient vector for these `weights` and training example `training[0]`?\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: grad_vector\n",
        "-->"
      ],
      "id": "eae24505"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "3880fbb1",
        "outputId": "b1e164a8-f93c-4830-f106-c532ad3a3052"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-2.6015, -1.7343, -0.5781, -0.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "#TODO: Calculate the gradient vector for these `weights` and training example `training[0]` \n",
        "grad_vector = (sigma(torch.dot(training[0]['counts'],weights)) - int(training[0]['authors']==\"Hamilton\")) * training[0]['counts']\n",
        "grad_vector"
      ],
      "id": "3880fbb1"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "7702abc3",
        "outputId": "478da6a3-5a88-4a08-c3a7-b295921ff66c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ],
      "source": [
        "grader.check(\"grad_vector\")"
      ],
      "id": "7702abc3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76041f8a"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Deriving the gradient of the cross-entropy loss\n",
        "\n",
        "> You should skip this section until after you've done the rest of the lab.\n",
        "\n",
        "The derivation for the cross-entropy loss relies on the salutary fact that the derivative of the sigmoid has an especially simple form:\n",
        "\n",
        "$$ \\frac{d \\sigma(z)}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z)) $$\n",
        "\n",
        "**Question:** Prove this identity. For simplicity, you can assume that $k=1$.\n",
        "\n",
        "> **Hint:** You may find some of the following standard formulas for the derivative of various functions – reviewed from your calculus course – useful. (In these schematic identities, $u$ and $v$ are metavariables over functions of $z$, and $a$ and $n$ are metavariables over constants.)\n",
        "\n",
        "\n",
        "\\begin{align*}\n",
        "\\frac{d}{dz} a &= 0 \\\\\n",
        "\\frac{d}{dz} (u + v) &= \\frac{du}{dz} + \\frac{dv}{dz} \\\\\n",
        "\\frac{d}{dz} (u\\,v) &= v\\frac{du}{dz} + u\\frac{dv}{dz} \\\\\n",
        "\\frac{d}{dz} \\left[\\frac{1}{u}\\right] &= - \\frac{1}{u^2} \\frac{du}{dz} \\\\\n",
        "\\frac{d}{dz} u^n &= n u^{n-1} \\frac{du}{dz} \\\\\n",
        "\\frac{d}{dz} e^u &= e^u \\frac{du}{dz} \\\\\n",
        "\\frac{d}{dz} \\log_a u &= (\\log_a e) \\frac{1}{u} \\frac{du}{dz} \n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "manual: true\n",
        "name: open_response_sigmoid_gradient\n",
        "-->"
      ],
      "id": "76041f8a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de49cd82"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "Defining the function –\n",
        "$$\\sigma(z) = \\frac{1}{1 + e^{-k z}}$$\n",
        "\n",
        "First, after performing algebraic simplifications –\n",
        "$$1-\\sigma(z) = 1-\\frac{1}{1 + e^{-k z}} = \\frac{e^{-k z}}{1 + e^{-k z}}$$\n",
        "\n",
        "After deriving the function –\n",
        "\\begin{align*}\n",
        "\\frac{d \\sigma(z)}{dz} = \\frac{d}{dz} \\left[\\frac{1}{u}\\right] &= - \\frac{1}{u^2} \\frac{du}{dz} |_{u=1 + e^{-k z}} = - \\frac{1}{(1 + e^{-k z})^2} \\cdot (-ke^{-k z})  = \\frac{ke^{-k z}}{(1 + e^{-k z})^2} = k\\sigma(z)(1-\\sigma(z)) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "And for $k=1$, the identity above holds –\n",
        "$$\\frac{d \\sigma(z)}{dz} = \\sigma(z) \\cdot (1 - \\sigma(z))$$\n",
        "\n",
        "$\\blacksquare$"
      ],
      "id": "de49cd82"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95adad52"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "## Adjusting weights against the gradient\n",
        "\n",
        "Step 2 is to adjust the weights in the _opposite_ direction of the gradient.\n",
        "In this case, we compute the new weight vector $\\mathbf{w}'$ by adding to the weight vector a fraction of the negative gradient:\n",
        "\n",
        "$$ \\mathbf{w}' = \\mathbf{w} - \\eta \\nabla L_{CE}(\\mathbf{w}) $$\n",
        "\n",
        "Here, $\\eta$ is the _learning rate_. The larger $\\eta$ is, the more we move in each step, but if $\\eta$ is too large we risk overshooting. We'll use a learning rate of 0.1 for now. (Setting good learning rates is one aspect of the black arts of machine learning.)"
      ],
      "id": "95adad52"
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "469e1a06"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.1"
      ],
      "id": "469e1a06"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81d7f613"
      },
      "source": [
        "Calculate the new weight vector using `learning_rate` and `grad_vector`. Use `weights` (instead of `weights_new`) as the initial weights.\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: weights_updated\n",
        "-->"
      ],
      "id": "81d7f613"
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "d2bcaf2a"
      },
      "outputs": [],
      "source": [
        "weights_updated = weights - learning_rate * grad_vector"
      ],
      "id": "d2bcaf2a"
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "084f0678",
        "outputId": "2b69b15d-312e-49ce-a960-87c236612755"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ],
      "source": [
        "grader.check(\"weights_updated\")"
      ],
      "id": "084f0678"
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "b9b7ac37",
        "outputId": "96dff704-7cda-4561-b4ad-cb9835dbad8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 0.1601,  0.3734,  0.3578, -1.0000])\n"
          ]
        }
      ],
      "source": [
        "print(weights_updated)"
      ],
      "id": "b9b7ac37"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a8807154"
      },
      "source": [
        "How do these weights perform on the training example we've been using? Let's see."
      ],
      "id": "a8807154"
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "2ddd3cbd"
      },
      "outputs": [],
      "source": [
        "loss_example0_updated = loss(weights_updated, training[0]['counts'], 1)"
      ],
      "id": "2ddd3cbd"
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "1cb98dd1",
        "outputId": "08f26696-0e3a-4ab8-a333-a8e74019e139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old model loss: 0.492\n",
            "New model loss: 0.094\n",
            "Updated model loss: 0.018\n"
          ]
        }
      ],
      "source": [
        "print(f\"Old model loss: {loss_example0_old:.3f}\\n\"\n",
        "      f\"New model loss: {loss_example0_new:.3f}\\n\"\n",
        "      f\"Updated model loss: {loss_example0_updated:.3f}\")"
      ],
      "id": "1cb98dd1"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5944ed97"
      },
      "source": [
        "If you did this all right, the loss for the updated model, which was generated by taking a single step opposite the gradient from the old model is not only better than the old model, but better than the new model we used above as well. \n",
        "\n",
        "What about the loss on the other example we've been using (example 9)? Calculate the loss for example 9 with the old model (`weights`), the new model (`weights_new`), and the updated model (`weights_updated`):\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: weights_updated_9\n",
        "-->"
      ],
      "id": "5944ed97"
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "9e8d0197"
      },
      "outputs": [],
      "source": [
        "#TODO\n",
        "loss_example9_old = loss(weights,training[9]['counts'],int(training[9]['authors']==\"Hamilton\"))\n",
        "loss_example9_new = loss(weights_new,training[9]['counts'],int(training[9]['authors']==\"Hamilton\"))\n",
        "loss_example9_updated = loss(weights_updated,training[9]['counts'],int(training[9]['authors']==\"Hamilton\"))"
      ],
      "id": "9e8d0197"
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 46
        },
        "id": "20567114",
        "outputId": "b4c5e671-7f75-4687-ba57-91fb408129c7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\n",
              "    All tests passed!\n",
              "    "
            ],
            "text/html": [
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "grader.check(\"weights_updated_9\")"
      ],
      "id": "20567114"
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f51fbf4b",
        "outputId": "785b051f-95ff-4760-aa04-8feaaf12a9f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Old model loss: 0.094\n",
            "New model loss: 0.052\n",
            "Updated model loss: 2.722\n"
          ]
        }
      ],
      "source": [
        "print(f\"Old model loss: {loss_example9_old:.3f}\\n\"\n",
        "      f\"New model loss: {loss_example9_new:.3f}\\n\"\n",
        "      f\"Updated model loss: {loss_example9_updated:.3f}\")"
      ],
      "id": "f51fbf4b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0b49ba4"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "**Question:** Did the update to the model improve its performance on example 9 or make it worse? Why?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_updated_9\n",
        "manual: true\n",
        "-->"
      ],
      "id": "f0b49ba4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a9eac1fd"
      },
      "source": [
        "**Answer:**\n",
        "\n",
        "The update to the model made the performance on example #9 worse. After performing the update, the loss is now higher, but we want the updates to minimize the loss. So the update did not improve the performance on example #9 and made it worse.\n",
        "\n",
        "The reason for it is that we performed an update that is locally optimizing the tested example #0, and such a locally optimizing step does not necessarily optimize the model globally for all the possible examples, as we see here."
      ],
      "id": "a9eac1fd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "36d7e4b6"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "## Repeating the process\n",
        "\n",
        "Step 3 is to repeat the process for this and other training examples. We could recalculate the gradient and take another step to improve further, and take steps to improve the other training examples, and so on and so forth, eventually converging on a model that works well over the entire training set. But doing so manually in this way is too tedious. We need to be able to do these kinds of computations at scale. Fortunately, in the next lab we'll be turning to packages that allow specifying these larger-scale computations."
      ],
      "id": "36d7e4b6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19aab134"
      },
      "source": [
        "<!-- BEGIN QUESTION -->\n",
        "\n",
        "## Lab debrief\n",
        "\n",
        "**Question:** We're interested in any thoughts you have about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n",
        "\n",
        "* Was the lab too long or too short?\n",
        "* Were the readings appropriate for the lab? \n",
        "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n",
        "* Are there additions or changes you think would make the lab better?\n",
        "\n",
        "<!--\n",
        "BEGIN QUESTION\n",
        "name: open_response_debrief\n",
        "manual: true\n",
        "-->"
      ],
      "id": "19aab134"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50fdf020"
      },
      "source": [
        "**Answer:**\n",
        "* No, it was appropriate for its time.\n",
        "* No, we found the reading irrelevant for the lab, yet insightful.\n",
        "* The intentions for some of the questions were not clear and we have asked many questions. However, the points of the excersises were clear.\n",
        "* We think the lab was good."
      ],
      "id": "50fdf020"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19ab42a8"
      },
      "source": [
        "<!-- END QUESTION -->\n",
        "\n",
        "\n",
        "\n",
        "# End of lab 1-4"
      ],
      "id": "19ab42a8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06b877a7"
      },
      "source": [
        "---\n",
        "\n",
        "To double-check your work, the cell below will rerun all of the autograder tests."
      ],
      "id": "06b877a7"
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "719c2c4a",
        "outputId": "a92829c7-4231-4806-c6cf-b1bf187d1bcb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "grad_vector:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "loss_example0_old_and_new:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "prob_hamilton_examples:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "prob_hamilton_examples_new:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "sigma:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "weights_updated:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "weights_updated_9:\n",
              "\n",
              "    All tests passed!\n",
              "    \n",
              "\n",
              "wtd_sum:\n",
              "\n",
              "    All tests passed!\n",
              "    \n"
            ],
            "text/html": [
              "<p><strong>grad_vector:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>loss_example0_old_and_new:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>prob_hamilton_examples:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>prob_hamilton_examples_new:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>sigma:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>weights_updated:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>weights_updated_9:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n",
              "<p><strong>wtd_sum:</strong></p>\n",
              "\n",
              "    \n",
              "    \n",
              "        <p>All tests passed!</p>\n",
              "    \n",
              "    \n",
              "\n"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "grader.check_all()"
      ],
      "id": "719c2c4a"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "title": "CS236299 Lab 1-4: Discriminative methods for classification"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}